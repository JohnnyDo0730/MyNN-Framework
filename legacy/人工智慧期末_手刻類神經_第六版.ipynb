{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 載入套件"],"metadata":{"id":"eP6wzHwPTgSH"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.utils.data import DataLoader, random_split\n","from torchvision import transforms\n","from torchvision.datasets import MNIST"],"metadata":{"id":"GdWy1csJzkKP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 參考架構 (MNIST 98%)"],"metadata":{"id":"mRHNSVZenwb3"}},{"cell_type":"code","source":["from torch.nn.modules.activation import ReLU\n","# 建立模型\n","class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.fc1 = torch.nn.Linear(28 * 28, 512) # 完全連接層\n","        self.relu1 = nn.ReLU()\n","        self.dropout1 = nn.Dropout(0.2)  # Dropout層\n","        self.fc2 = torch.nn.Linear(512, 128) # 完全連接層\n","        self.relu2 = nn.ReLU()\n","        self.fc3 = torch.nn.Linear(128, 10) # 完全連接層\n","\n","    def forward(self, x):\n","        x = torch.flatten(x, 1)\n","        x = self.fc1(x)\n","        x = self.relu1(x)\n","        x = self.dropout1(x)\n","        x = self.fc2(x)\n","        x = self.relu2(x)\n","        x = self.fc3(x)\n","        return x\n","\n","# 建立模型物件\n","model = Net().to(device)"],"metadata":{"id":"yMxqUNAVnzKq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 手刻類神經"],"metadata":{"id":"IjiN2hH7Rlrv"}},{"cell_type":"markdown","source":["## 第一版\n","    backword中更新權重"],"metadata":{"id":"hXUPPAf0sX8r"}},{"cell_type":"markdown","source":["### 定義layer結構"],"metadata":{"id":"fzUtXppHSDM5"}},{"cell_type":"code","source":["#定義全連接層結構\n","class myLinear():\n","  def __init__(self, input_size, output_size,learning_rate):\n","        #初始化參數\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.learning_rate = learning_rate\n","        self.x = None\n","\n","        #初始化權重\n","        self.W = np.random.uniform(-1, 1, (output_size, input_size))\n","        self.b = np.zeros((output_size,1))\n","\n","        return\n","\n","  def forward(self, x):\n","        self.x = x # 儲存本次計算結果\n","        return np.dot(self.W, x) + self.b\n","\n","  def backward(self, error):\n","        gradient_transfer = np.dot(self.W.T, error) # 梯度傳遞\n","        self.optimization(error) # 更新權重\n","\n","        return gradient_transfer\n","\n","  def optimization(self, error):\n","        batch_size = self.x.shape[1]  # Batch 大小\n","\n","        # 計算權重和偏差的梯度平均值\n","        dW = np.dot(error, self.x.T) / batch_size\n","        db = np.sum(error, axis=1, keepdims=True) / batch_size\n","\n","        # 更新權重和偏差\n","        self.W -= self.learning_rate * dW\n","        self.b -= self.learning_rate * db\n","\n","        return"],"metadata":{"id":"XoVll3bgTUd3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#定義dropout層\n","import numpy as np\n","\n","class myDropout:\n","  def __init__(self, p=0.5):\n","        self.p = p\n","        self.mask = None\n","\n","        return\n","\n","  def forward(self, x, training=True):\n","\n","        if not training:\n","            return x  # 測試時直接返回輸入\n","\n","        # 隨機生成遮罩\n","        self.mask = np.random.rand(*x.shape) > self.p\n","        # 掩碼應用並進行縮放\n","        return x * self.mask / (1 - self.p)\n","\n","  def backward(self, error):\n","\n","        # 只保留激活的部分\n","        return error * self.mask\n"],"metadata":{"id":"ULLWTKxXjf6I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#定義activation function\n","\n","#ReLu\n","class myReLU():\n","  def __init__(self):\n","        self.x = None\n","        return\n","\n","  def forward(self, x):\n","        self.x = x\n","        return np.maximum(0, x)\n","\n","  def backward(self, error):\n","\n","        return error * (self.x > 0)"],"metadata":{"id":"uD9JDuvOl6qL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 定義神經網路架構"],"metadata":{"id":"y7yo41NAkFuw"}},{"cell_type":"code","source":["#note set training state, learning rate\n","\n","class myNet():\n","  def __init__(self, learning_rate):\n","        self.learning_rate = learning_rate\n","        self.train_state = True\n","\n","        self.fc1 = myLinear(28*28, 512, learning_rate)\n","        self.relu1 = myReLU()\n","        self.dropout1 = myDropout(0.2)\n","        self.fc2 = myLinear(512, 128, learning_rate)\n","        self.relu2 = myReLU()\n","        self.fc3 = myLinear(128, 10, learning_rate)\n","\n","  def set_train_state(self, state):\n","        self.train_state = state\n","\n","  def forward(self, x):\n","        x = x.reshape(x.shape[0], -1)\n","        x = self.fc1.forward(x)\n","        x = self.relu1.forward(x)\n","        x = self.dropout1.forward(x, self.train_state)\n","        x = self.fc2.forward(x)\n","        x = self.relu2.forward(x)\n","        x = self.fc3.forward(x)\n","        return x\n","\n","  def backward(self, error):\n","        error = self.fc3.backward(error)\n","        error = self.relu2.backward(error)\n","        error = self.fc2.backward(error)\n","        error = self.relu1.backward(error)\n","        error = self.dropout1.backward(error)\n","        error = self.fc1.backward(error)\n","        return error"],"metadata":{"id":"p_z6WJSvsJmA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 第二版\n","    1.將梯度更新抽離backward、整合到optimizer抽象類\n","    2.loss計算適配batch更新\n","    3.實現Cross Entropy Loss計算"],"metadata":{"id":"PulNPF_VsqZw"}},{"cell_type":"markdown","source":["### 定義layer結構"],"metadata":{"id":"UWSERN1nV7_B"}},{"cell_type":"code","source":["#定義全連接層結構\n","class myLinear():\n","  def __init__(self, input_size, output_size):\n","        #初始化參數\n","        self.input_size = input_size\n","        self.output_size = output_size\n","\n","        # 用于存储梯度\n","        self.x = None\n","        self.dW = None\n","        self.db = None\n","\n","        #初始化權重\n","        self.W = np.random.uniform(-1, 1, (output_size, input_size))\n","        self.b = np.zeros((output_size,1))\n","        return\n","\n","  def forward(self, x):\n","        self.x = x # 儲存本次計算結果\n","        return np.dot(self.W, x) + self.b\n","\n","  def backward(self, error):\n","\n","        # 計算自己的更新梯度\n","        self.dW = np.dot(error, self.x.T)\n","        self.db = np.sum(error, axis=1, keepdims=True)\n","\n","        # 梯度傳遞\n","        gradient_transfer = np.dot(self.W.T, error)\n","        return gradient_transfer\n"],"metadata":{"id":"t9TeGslVV7_B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#定義dropout層\n","\n","class myDropout:\n","  def __init__(self, p=0.5):\n","        self.p = p\n","        self.mask = None\n","        return\n","\n","  def forward(self, x, training=True):\n","\n","        if not training:\n","            return x  # 測試時直接返回輸入\n","\n","        # 隨機生成遮罩\n","        self.mask = np.random.rand(*x.shape) > self.p\n","        # 遮罩應用並進行縮放\n","        return x * self.mask / (1 - self.p)\n","\n","  def backward(self, error):\n","\n","        # 只保留激活的部分\n","        return error * self.mask\n"],"metadata":{"id":"Q2TbwU_dV7_B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#定義activation function\n","\n","#ReLu\n","class myReLU():\n","  def __init__(self):\n","        self.x = None\n","        return\n","\n","  def forward(self, x):\n","        self.x = x\n","        return np.maximum(0, x)\n","\n","  def backward(self, error):\n","        return error *  (self.x > 0).astype(float)"],"metadata":{"id":"Cg0tnux_V7_B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 定義神經網路架構"],"metadata":{"id":"L6SoFkhfV7_B"}},{"cell_type":"code","source":["#定義Net架構\n","class myNet():\n","  def __init__(self):\n","        self.train_state = True\n","\n","        self.fc1 = myLinear(28*28, 512)\n","        self.relu1 = myReLU()\n","        self.dropout1 = myDropout(0.2)\n","        self.fc2 = myLinear(512, 128)\n","        self.relu2 = myReLU()\n","        self.fc3 = myLinear(128, 10)\n","\n","  def set_train_state(self, state):\n","        self.train_state = state\n","\n","  def forward(self, x):\n","        x = torch.flatten(x, 1).T\n","        x = self.fc1.forward(x)\n","        x = self.relu1.forward(x)\n","        x = self.dropout1.forward(x, self.train_state)\n","        x = self.fc2.forward(x)\n","        x = self.relu2.forward(x)\n","        x = self.fc3.forward(x)\n","        return x\n","\n","  def backward(self, error):\n","        error = self.fc3.backward(error)\n","        error = self.relu2.backward(error)\n","        error = self.fc2.backward(error)\n","        error = self.relu1.backward(error)\n","        error = self.dropout1.backward(error)\n","        error = self.fc1.backward(error)\n","        return error\n","\n","  def get_layers(self):\n","        # 返回需要更新權重的層\n","        return [self.fc1, self.fc2, self.fc3]"],"metadata":{"id":"nqXYOU-0V7_B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 定義優化相關結構"],"metadata":{"id":"Arm9rZkXpbFy"}},{"cell_type":"code","source":["#定義Optimizer類\n","\n","#抽象類\n","class Optimizer:\n","    def __init__(self, learning_rate):\n","        self.learning_rate = learning_rate\n","\n","    def step(self, layers):\n","        raise NotImplementedError(\"Subclasses should implement this!\")\n","\n","#繼承類\n","class SGD(Optimizer):\n","    def __init__(self, learning_rate):\n","        super().__init__(learning_rate)\n","\n","    def step(self, layers, batch_size):\n","        for layer in layers:\n","            layer.W -= self.learning_rate * (layer.dW / batch_size)\n","            layer.b -= self.learning_rate * (layer.db / batch_size)\n","\n","class Momentum(Optimizer):\n","    def __init__(self, learning_rate, momentum=0.9):\n","        super().__init__(learning_rate)\n","        self.momentum = momentum\n","        self.velocities = {}\n","\n","    def step(self, layers):\n","        for layer in layers:\n","            if layer not in self.velocities:\n","                self.velocities[layer] = {\"W\": np.zeros_like(layer.W), \"b\": np.zeros_like(layer.b)}\n","            v = self.velocities[layer]\n","            # 更新動量\n","            v[\"W\"] = self.momentum * v[\"W\"] + self.learning_rate * layer.dW\n","            v[\"b\"] = self.momentum * v[\"b\"] + self.learning_rate * layer.db\n","            # 更新參數\n","            layer.W -= v[\"W\"]\n","            layer.b -= v[\"b\"]"],"metadata":{"id":"hGX9EWfhX3KE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def softmax(logits):\n","    # 防止溢出，减去最大值\n","    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # Change axis to 1\n","    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)  # Change axis to 1\n","\n","def cross_entropy_loss(outputs, targets):\n","    # 计算 softmax\n","    probs = softmax(outputs) # Transpose outputs to (num_samples, num_classes)\n","    #print(np.sum(probs,axis=1, keepdims=True))\n","\n","    # 将 targets 转换为 NumPy 数组并确保其类型为 int\n","    targets = targets.cpu().numpy().astype(int)\n","\n","    # 创建 one-hot 编码 (修正索引方式)\n","    num_samples = targets.shape[0]\n","    num_classes = probs.shape[1]\n","    targets_one_hot = np.zeros((num_samples, num_classes))\n","    targets_one_hot[np.arange(num_samples), targets] = 1\n","\n","    # 计算交叉熵损失\n","    loss = -np.sum(targets_one_hot * np.log(probs + 1e-15)) / num_samples\n","    grad = probs - targets_one_hot\n","    return loss, grad\n"],"metadata":{"id":"de3ARocBYKK7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 第三版\n","    1.np運算改成tensor運算，適配GPU訓練"],"metadata":{"id":"YYIzI7rQbYpZ"}},{"cell_type":"markdown","source":["### 定義layer結構"],"metadata":{"id":"Qb7tCIeQbYpa"}},{"cell_type":"code","source":["#定義全連接層結構\n","class myLinear():\n","  def __init__(self, input_size, output_size, device='cpu'):\n","        #初始化參數\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.device = device\n","\n","        # 用于存储梯度\n","        self.x = None\n","        self.dW = None\n","        self.db = None\n","\n","        # 初始化權重并移到指定设备\n","        self.W = torch.randn(output_size, input_size, device=self.device)\n","        self.b = torch.zeros(output_size, 1, device=self.device)\n","        return\n","\n","  def forward(self, x):\n","        self.x = x.to(self.device) # 儲存本次計算結果\n","        return torch.matmul(self.W, x) + self.b\n","\n","  def backward(self, error):\n","\n","        # 計算自己的更新梯度\n","        self.dW = torch.matmul(error, self.x.T)\n","        self.db = torch.sum(error, dim=1, keepdim=True)\n","\n","        # 梯度傳遞\n","        gradient_transfer = torch.matmul(self.W.T, error)\n","        return gradient_transfer\n","\n","  def to(self, device):\n","        # 将参数转移到设备\n","        self.device = device\n","        self.W = self.W.to(device)\n","        self.b = self.b.to(device)"],"metadata":{"id":"jNkMh8enbYpa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#定義dropout層\n","\n","class myDropout:\n","  def __init__(self, p=0.5):\n","        self.p = p\n","        self.mask = None\n","        return\n","\n","  def forward(self, x, training=True):\n","        if not training:\n","            return x  # 測試時直接返回輸入\n","\n","        # 隨機生成遮罩\n","        self.mask = torch.rand_like(x) > self.p\n","        # 遮罩應用並進行縮放\n","        return x * self.mask / (1 - self.p)\n","\n","  def backward(self, error):\n","\n","        # 只保留激活的部分\n","        return error * self.mask\n"],"metadata":{"id":"5cJ6t8ixbYpa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#定義activation function\n","\n","#ReLu\n","class myReLU():\n","  def __init__(self):\n","        self.x = None\n","        return\n","\n","  def forward(self, x):\n","        self.x = x\n","        return torch.maximum(torch.tensor(0.0).to(x.device), x)\n","\n","  def backward(self, error):\n","        return error * (self.x > 0).float()"],"metadata":{"id":"KWYeIFaSbYpa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 定義神經網路架構"],"metadata":{"id":"TptZ0u-ZbYpa"}},{"cell_type":"code","source":["#定義Net架構\n","class myNet():\n","  def __init__(self):\n","        self.train_state = True\n","\n","        self.fc1 = myLinear(28*28, 512)\n","        self.relu1 = myReLU()\n","        self.dropout1 = myDropout(0.2)\n","        self.fc2 = myLinear(512, 128)\n","        self.relu2 = myReLU()\n","        self.fc3 = myLinear(128, 10)\n","\n","        self.NNlayers = [self.fc1, self.fc2, self.fc3]\n","\n","  def set_train_state(self, state):\n","        self.train_state = state\n","\n","  def forward(self, x):\n","        x = torch.flatten(x, 1).T # [batch, gray, x, y] > [x*y, batch]\n","        x = self.fc1.forward(x)\n","        x = self.relu1.forward(x)\n","        x = self.dropout1.forward(x, self.train_state)\n","        x = self.fc2.forward(x)\n","        x = self.relu2.forward(x)\n","        x = self.fc3.forward(x)\n","        return x\n","\n","  def backward(self, error):\n","        error = self.fc3.backward(error)\n","        error = self.relu2.backward(error)\n","        error = self.fc2.backward(error)\n","        error = self.relu1.backward(error)\n","        error = self.dropout1.backward(error)\n","        error = self.fc1.backward(error)\n","        return error\n","\n","  def get_layers(self):\n","        # 返回需要更新權重的層\n","        return self.NNlayers\n","\n","  def to(self, device):\n","        for layer in self.NNlayers:\n","            layer.to(device)\n","\n","        return self"],"metadata":{"id":"VO7LhBXNbYpa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 定義優化相關結構"],"metadata":{"id":"LnDhud4RpZwy"}},{"cell_type":"code","source":["#定義Optimizer類\n","\n","#抽象類\n","class Optimizer:\n","    def __init__(self, learning_rate):\n","        self.learning_rate = learning_rate\n","\n","    def step(self, layers):\n","        raise NotImplementedError(\"Subclasses should implement this!\")\n","\n","#繼承類\n","class SGD(Optimizer):\n","    def __init__(self, learning_rate):\n","        super().__init__(learning_rate)\n","\n","    def step(self, layers, batch_size):\n","        for layer in layers:\n","            layer.W -= self.learning_rate * (layer.dW / batch_size)\n","            layer.b -= self.learning_rate * (layer.db / batch_size)\n","\n","class Momentum(Optimizer):\n","    def __init__(self, learning_rate, momentum=0.9):\n","        super().__init__(learning_rate)\n","        self.momentum = momentum\n","        self.velocities = {}\n","\n","    def step(self, layers):\n","        for layer in layers:\n","            if layer not in self.velocities:\n","                self.velocities[layer] = {\"W\": torch.zeros_like(layer.W), \"b\": torch.zeros_like(layer.b)}\n","            v = self.velocities[layer]\n","            # 更新動量\n","            v[\"W\"] = self.momentum * v[\"W\"] + self.learning_rate * layer.dW\n","            v[\"b\"] = self.momentum * v[\"b\"] + self.learning_rate * layer.db\n","            # 更新參數\n","            layer.W -= v[\"W\"]\n","            layer.b -= v[\"b\"]"],"metadata":{"id":"opHfHN8ObYpb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def softmax(logits):\n","    # 防止溢出，减去最大值\n","    exp_logits = torch.exp(logits - torch.max(logits, dim=1, keepdim=True)[0]) # 取最大值并防止溢出\n","    return exp_logits / torch.sum(exp_logits, dim=1, keepdim=True)  # 使用 torch.sum 计算归一化\n","\n","def cross_entropy_loss(outputs, targets):\n","    # 计算 softmax\n","    probs = softmax(outputs) # Transpose outputs to (num_samples, num_classes)\n","    #print(np.sum(probs,axis=1, keepdims=True))\n","\n","    # 将 targets 转换为 NumPy 数组并确保其类型为 int\n","    #targets = targets.cpu().numpy().astype(int)\n","\n","    # 创建 one-hot 编码 (修正索引方式)\n","    num_samples = targets.shape[0]\n","    num_classes = probs.shape[1]\n","    targets_one_hot = torch.zeros(num_samples, num_classes, device=outputs.device)\n","    targets_one_hot.scatter_(1, targets.unsqueeze(1), 1)  # 使用 scatter_ 创建 one-hot 编码\n","\n","    # 计算交叉熵损失\n","    loss = -torch.sum(targets_one_hot * torch.log(probs + 1e-15)) / num_samples\n","    grad = probs - targets_one_hot\n","    return loss, grad\n"],"metadata":{"id":"xMouOYr_bYpb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 第四版\n","    1.維度反轉操作移到模型內，使操作與pyTorch對齊\n","    2.將tensor精度從float32改成float64(與np相同)改善精度問題\n","    3.新增leaky ReLU、初始化使用uniform改善梯度消失問題\n","    4..新增Adam optimizer"],"metadata":{"id":"9Nov-1RFpJNq"}},{"cell_type":"markdown","source":["### 定義layer結構"],"metadata":{"id":"gLix9lw1pJNr"}},{"cell_type":"code","source":["#定義全連接層結構\n","class myLinear():\n","  def __init__(self, input_size, output_size, device='cpu',dtype=torch.float64):\n","        #初始化參數\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.device = device\n","        self.dtype = dtype\n","\n","        # 用于存储梯度\n","        self.x = None\n","        self.dW = None\n","        self.db = None\n","\n","        # 初始化權重并移到指定设备\n","        #self.W = torch.randn(output_size, input_size, device=self.device, dtype=self.dtype)\n","        self.W = 2 * torch.rand(output_size, input_size, device=self.device, dtype=self.dtype) - 1\n","        self.b = torch.zeros(output_size, 1, device=self.device, dtype=self.dtype)\n","        return\n","\n","  def forward(self, x):\n","        self.x = x.to(self.device, dtype=self.dtype) # 儲存本次計算結果\n","        return torch.matmul(self.W, self.x) + self.b\n","\n","  def backward(self, error):\n","\n","        # 計算自己的更新梯度\n","        self.dW = torch.matmul(error, self.x.T)\n","        self.db = torch.sum(error, dim=1, keepdim=True)\n","\n","        # 梯度傳遞\n","        gradient_transfer = torch.matmul(self.W.T, error)\n","        return gradient_transfer\n","\n","  def to(self, device):\n","        # 将参数转移到设备\n","        self.device = device\n","        self.W = self.W.to(device)\n","        self.b = self.b.to(device)"],"metadata":{"id":"Htq2OL_apJNr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#定義dropout層\n","\n","class myDropout:\n","  def __init__(self, p=0.5, dtype=torch.float64):\n","        self.p = p\n","        self.mask = None\n","        self.dtype = dtype\n","        return\n","\n","  def forward(self, x, training=True):\n","        if not training:\n","            return x  # 測試時直接返回輸入\n","\n","        # 隨機生成遮罩\n","        self.mask = torch.rand_like(x, dtype=self.dtype) > self.p\n","        # 遮罩應用並進行縮放\n","        return x * self.mask / (1 - self.p)\n","\n","  def backward(self, error):\n","\n","        # 只保留激活的部分\n","        return error * self.mask\n"],"metadata":{"id":"nt9FoAs9pJNr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#定義activation function\n","\n","#ReLu\n","class myReLU():\n","  def __init__(self):\n","        self.x = None\n","        return\n","\n","  def forward(self, x):\n","        self.x = x\n","        return torch.maximum(torch.tensor(0.0, dtype=x.dtype).to(x.device), x)\n","\n","  def backward(self, error):\n","        # ReLU(x)' = x>0 ? 1:0\n","        return error * (self.x > 0).float()\n","\n","# LeakyReLU\n","class myLeakyReLU():\n","  def __init__(self, alpha=0.01):\n","        self.x = None\n","        self.alpha = alpha  # 控制負區域的斜率\n","        return\n","\n","  def forward(self, x):\n","        self.x = x\n","        return torch.where(x > 0, x, self.alpha * x)  # 若 x > 0，則輸出 x，否則輸出 alpha * x\n","\n","  def backward(self, error):\n","        # Leaky ReLU(x)' = x > 0 ? 1 : alpha\n","        return error * torch.where(self.x > 0, torch.ones_like(self.x), self.alpha * torch.ones_like(self.x))"],"metadata":{"id":"2-wSrZYzpJNr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 定義神經網路架構"],"metadata":{"id":"49qOVo-VpJNr"}},{"cell_type":"code","source":["#定義Net架構\n","class myNet():\n","  def __init__(self):\n","        self.train_state = True\n","\n","        self.fc1 = myLinear(28*28, 512)\n","        self.relu1 = myLeakyReLU()\n","        self.dropout1 = myDropout(0.2)\n","        self.fc2 = myLinear(512, 128)\n","        self.relu2 = myLeakyReLU()\n","        self.fc3 = myLinear(128, 10)\n","\n","        self.NNlayers = [self.fc1, self.fc2, self.fc3]\n","\n","  def set_train_state(self, state):\n","        self.train_state = state\n","\n","  def forward(self, x):\n","        x = torch.flatten(x, 1).T # [batch, gray, x, y] => [x*y, batch]\n","        x = self.fc1.forward(x)\n","        x = self.relu1.forward(x)\n","        x = self.dropout1.forward(x, self.train_state)\n","        x = self.fc2.forward(x)\n","        x = self.relu2.forward(x)\n","        x = self.fc3.forward(x)\n","        return x\n","\n","  def backward(self, error):\n","        error = error.T\n","        error = self.fc3.backward(error)\n","        error = self.relu2.backward(error)\n","        error = self.fc2.backward(error)\n","        error = self.relu1.backward(error)\n","        error = self.dropout1.backward(error)\n","        error = self.fc1.backward(error)\n","        return error\n","\n","  def get_layers(self):\n","        # 返回需要更新權重的層\n","        return self.NNlayers\n","\n","  def to(self, device):\n","        for layer in self.NNlayers:\n","            layer.to(device)\n","\n","        return self"],"metadata":{"id":"waX7q7YqpJNr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 定義優化相關結構"],"metadata":{"id":"P8RALqxxrKAH"}},{"cell_type":"code","source":["#定義Optimizer類\n","\n","#抽象類\n","class Optimizer:\n","    def __init__(self, learning_rate):\n","        self.learning_rate = learning_rate\n","\n","    def step(self, layers):\n","        raise NotImplementedError(\"Subclasses should implement this!\")\n","\n","#繼承類\n","class SGD(Optimizer):\n","    def __init__(self, learning_rate):\n","        super().__init__(learning_rate)\n","\n","    def step(self, layers, batch_size):\n","        for layer in layers:\n","            layer.W -= self.learning_rate * (layer.dW / batch_size)\n","            layer.b -= self.learning_rate * (layer.db / batch_size)\n","\n","class Momentum(Optimizer):\n","    def __init__(self, learning_rate, momentum=0.9):\n","        super().__init__(learning_rate)\n","        self.momentum = momentum\n","        self.velocities = {}\n","\n","    def step(self, layers,batch_size):\n","        for layer in layers:\n","            if layer not in self.velocities:\n","                self.velocities[layer] = {\"W\": torch.zeros_like(layer.W, dtype=layer.W.dtype), \"b\": torch.zeros_like(layer.b, dtype=layer.b.dtype)}\n","            v = self.velocities[layer]\n","            # 更新動量\n","            v[\"W\"] = self.momentum * v[\"W\"] + self.learning_rate * (layer.dW / batch_size)\n","            v[\"b\"] = self.momentum * v[\"b\"] + self.learning_rate * (layer.db / batch_size)\n","            # 更新參數\n","            layer.W -= v[\"W\"]\n","            layer.b -= v[\"b\"]\n","\n","\n","#Adam (chatGPT生成的)\n","class Adam(Optimizer):\n","    def __init__(self, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8):\n","        super().__init__(learning_rate)\n","        self.beta1 = beta1\n","        self.beta2 = beta2\n","        self.epsilon = epsilon\n","        self.m = {}  # 一階矩\n","        self.v = {}  # 二階矩\n","        self.t = 0  # 時間步長\n","\n","    def step(self, layers, batch_size):\n","        self.t += 1  # 每一步增加時間步長\n","        for layer in layers:\n","            # 初始化一階矩和二階矩\n","            if layer not in self.m:\n","                self.m[layer] = {\"W\": torch.zeros_like(layer.W), \"b\": torch.zeros_like(layer.b)}\n","                self.v[layer] = {\"W\": torch.zeros_like(layer.W), \"b\": torch.zeros_like(layer.b)}\n","\n","            # 更新一階矩和二階矩\n","            self.m[layer][\"W\"] = self.beta1 * self.m[layer][\"W\"] + (1 - self.beta1) * layer.dW / batch_size\n","            self.m[layer][\"b\"] = self.beta1 * self.m[layer][\"b\"] + (1 - self.beta1) * layer.db / batch_size\n","            self.v[layer][\"W\"] = self.beta2 * self.v[layer][\"W\"] + (1 - self.beta2) * layer.dW ** 2 / batch_size\n","            self.v[layer][\"b\"] = self.beta2 * self.v[layer][\"b\"] + (1 - self.beta2) * layer.db ** 2 / batch_size\n","\n","            # 偏置修正\n","            m_hat_W = self.m[layer][\"W\"] / (1 - self.beta1 ** self.t)\n","            m_hat_b = self.m[layer][\"b\"] / (1 - self.beta1 ** self.t)\n","            v_hat_W = self.v[layer][\"W\"] / (1 - self.beta2 ** self.t)\n","            v_hat_b = self.v[layer][\"b\"] / (1 - self.beta2 ** self.t)\n","\n","            # 更新參數\n","            layer.W -= self.learning_rate * m_hat_W / (torch.sqrt(v_hat_W) + self.epsilon)\n","            layer.b -= self.learning_rate * m_hat_b / (torch.sqrt(v_hat_b) + self.epsilon)\n"],"metadata":{"id":"EIg4vUWnpJNr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#定義損失函數\n","\n","#MSELoss\n","def mse_loss(outputs, targets):\n","    error = outputs - targets\n","    loss = torch.mean(torch.square(error))/2\n","    grad = error\n","    return loss, grad\n","\n","#CrossEntropyLoss\n","def softmax(logits):\n","    # 防止溢出，减去最大值\n","    exp_logits = torch.exp(logits - torch.max(logits, dim=1, keepdim=True)[0]) # 取最大值并防止溢出\n","    return exp_logits / torch.sum(exp_logits, dim=1, keepdim=True)  # 使用 torch.sum 计算归一化\n","\n","def cross_entropy_loss(outputs, targets):\n","    # 计算 softmax\n","    probs = softmax(outputs.T)\n","    #print(np.sum(probs,axis=1, keepdims=True))\n","\n","\n","\n","    # 创建 one-hot 编码 (修正索引方式)\n","    num_samples = targets.shape[0]\n","    num_classes = probs.shape[1]\n","    targets_one_hot = torch.zeros(num_samples, num_classes, device=outputs.device, dtype=outputs.dtype)\n","    targets_one_hot.scatter_(1, targets.unsqueeze(1), 1)  # 使用 scatter_ 创建 one-hot 编码\n","    #print(targets_one_hot)\n","\n","    # 计算交叉熵损失\n","    loss = -torch.sum(targets_one_hot * torch.log(probs + 1e-15)) / num_samples\n","    grad = probs - targets_one_hot\n","    return loss, grad\n"],"metadata":{"id":"KVpM5gTmpJNs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 第五版\n","    1.Net改成繼承Module抽象類\n","    2.覆寫__setattr__實現自動註冊layers，添加is_Layer、is_WeightLayer布林值\n","    3.承2.，抽象化Activation Function類\n","    4.承2.，根據is_WeightLayer更改Optimizer類邏輯\n","    5.承4.，將batch計算邏輯移到myLinear內\n","    6.新增sigmoid,tanh,MSELoss等時序相關方法"],"metadata":{"id":"C9NnRPx0OyOo"}},{"cell_type":"markdown","source":["### 定義layer結構"],"metadata":{"id":"Z5509j1BOyOo"}},{"cell_type":"code","source":["# 定義全連接層結構\n","class myLinear():\n","  def __init__(self, input_size, output_size, device='cpu',dtype=torch.float64):\n","        # 初始化參數\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.device = device\n","        self.dtype = dtype\n","\n","        self.isLayer = True\n","        self.isWeightLayer = True\n","\n","        # 用于儲存梯度\n","        self.x = None\n","        self.dW = None\n","        self.db = None\n","\n","        # 初始化權重並移到設備\n","        #self.W = torch.randn(output_size, input_size, device=self.device, dtype=self.dtype)\n","        self.W = 2 * torch.rand(output_size, input_size, device=self.device, dtype=self.dtype) - 1 # -1到1之間均勻分布\n","        self.b = torch.zeros(output_size, 1, device=self.device, dtype=self.dtype)\n","        return\n","\n","  def forward(self, x):\n","        self.x = x.to(self.device, dtype=self.dtype) # 儲存本次計算結果\n","        return torch.matmul(self.W, self.x) + self.b\n","\n","  def backward(self, error):\n","        # 計算batch_size\n","        batch_size = error.size(1)\n","\n","        # 計算自己的更新梯度 (將batch計算邏輯移到此處)\n","        self.dW = torch.matmul(error, self.x.T)/batch_size\n","        self.db = torch.sum(error, dim=1, keepdim=True)/batch_size\n","\n","        # 梯度傳遞\n","        gradient_transfer = torch.matmul(self.W.T, error)\n","        return gradient_transfer\n","\n","  def to(self, device):\n","        # 将参数轉移到設備\n","        self.device = device\n","        self.W = self.W.to(device)\n","        self.b = self.b.to(device)"],"metadata":{"id":"7DP8fgIoOyOo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class myLinear():\n","  ...\n","\n","  def backward(self, error):\n","        # 計算batch_size\n","        batch_size = error.size(1)\n","\n","        # 計算自己的更新梯度\n","        self.dW = torch.matmul(error, self.x.T)/batch_size\n","        self.db = torch.sum(error, dim=1, keepdim=True)/batch_size\n","\n","        # 梯度傳遞\n","        gradient_transfer = torch.matmul(self.W.T, error)\n","        return gradient_transfer"],"metadata":{"id":"DRzPIdhroNL8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 定義dropout層\n","class myDropout():\n","  def __init__(self, p=0.5, dtype=torch.float64):\n","        self.p = p\n","        self.mask = None\n","        self.dtype = dtype\n","\n","        self.isLayer = True\n","        self.isWeightLayer = False\n","        return\n","\n","  def forward(self, x, is_training=True):\n","        if not is_training:\n","            return x  # 測試時直接返回輸入\n","\n","        # 隨機生成遮罩\n","        self.mask = torch.rand_like(x, dtype=self.dtype) > self.p\n","        # 遮罩應用並進行縮放\n","        return x * self.mask / (1 - self.p)\n","\n","  def backward(self, error):\n","\n","        # 只保留激活的部分\n","        return error * self.mask\n"],"metadata":{"id":"5f5LXEJ7OyOp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 定義activation function類\n","\n","# 抽象類\n","class myActivation():\n","  def __init__(self):\n","        self.x = None\n","\n","        self.isLayer = True\n","        self.isWeightLayer = False\n","        return\n","\n","  def forward(self, x):\n","        raise NotImplementedError(\"Subclasses should implement this!\")\n","\n","  def backward(self, error):\n","        raise NotImplementedError(\"Subclasses should implement this!\")\n","\n","# ReLu\n","class myReLU(myActivation):\n","  def __init__(self):\n","        super().__init__()\n","        return\n","\n","  def forward(self, x):\n","        # ReLU(x) = x>0 ? x : 0\n","        self.x = x\n","        return torch.maximum(torch.tensor(0.0, dtype=x.dtype).to(x.device), x)\n","\n","  def backward(self, error):\n","        # ReLU(x)' = x>0 ? 1:0\n","        return error * (self.x > 0).float()\n","\n","# LeakyReLU\n","class myLeakyReLU(myActivation):\n","  def __init__(self, alpha=0.01):\n","        super().__init__()\n","\n","        self.alpha = alpha  # 控制負區域的斜率\n","        return\n","\n","  def forward(self, x):\n","        # Leaky ReLU(x) = x>0 ? x : alpha*x\n","        self.x = x\n","        return torch.where(x > 0, x, self.alpha * x)\n","\n","  def backward(self, error):\n","        # Leaky ReLU(x)' = x > 0 ? 1 : alpha\n","        return error * torch.where(self.x > 0, torch.ones_like(self.x), self.alpha * torch.ones_like(self.x))\n","\n","# Sigmoid (使用絕對數值、歸一化到[0,1]時使用)\n","class mySigmoid(myActivation):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.output = None\n","        return\n","\n","    def forward(self, x):\n","        # self.output = 1 / (1 + np.exp(-x))\n","        self.output = torch.sigmoid(x)\n","        return self.output\n","\n","    def backward(self, error):\n","        # sigmoid(x)' = sigmoid(x) * (1 - sigmoid(x))\n","        return error * self.output * (1 - self.output)\n","\n","\n","# Tanh (使用差分、標準化到[-1,1]時使用)\n","class myTanh(myActivation):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.output = None\n","        return\n","\n","    def forward(self, x):\n","        # self.output = np.tanh(x)\n","        self.output = torch.tanh(x)\n","        return self.output\n","\n","    def backward(self, error):\n","        # tanh(x)' = 1 - tanh^2(x)\n","        return error * (1 - self.output ** 2)"],"metadata":{"id":"KfBMNAq4OyOp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 定義神經網路結構"],"metadata":{"id":"B-Z_f0rgOyOp"}},{"cell_type":"code","source":["# 定義Module類\n","\n","# 抽象類\n","class myModule():\n","  def __init__(self):\n","      self.train_state = True\n","      self.layers = []\n","\n","  def set_train_state(self, state):\n","        self.train_state = state\n","\n","  def forward(self, x):\n","        raise NotImplementedError(\"Subclasses should implement this!\")\n","\n","  def backward(self, error):\n","        error = error.T # 輸入維度從pyTorch batch計算轉換成chain rule公式形式\n","        for layer in reversed(self.layers):\n","            error = layer.backward(error)\n","\n","  def get_layers(self):\n","        return self.layers\n","\n","  def to(self, device):\n","        for layer in self.layers:\n","          if hasattr(layer, 'is_WeightLayer'): # 只需搬動有權重的層\n","            layer.to(device)\n","\n","        return self\n","\n","  def __setattr__(self, name, value):\n","      if hasattr(value, 'is_layer') and value.is_layer:  # 有 is_layer 屬性才加入layers\n","          self.layers.append(value)\n","\n","      super().__setattr__(name, value) # 預設方法、建立任何子物件都要執行"],"metadata":{"id":"7ie_VTFlPRnZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 定義Net架構繼承Module類\n","class myNet(myModule):\n","  def __init__(self):\n","        super().__init__()\n","        self.fc1 = myLinear(28*28, 512)\n","        self.relu1 = myLeakyReLU()\n","        self.dropout1 = myDropout(0.2)\n","        self.fc2 = myLinear(512, 128)\n","        self.relu2 = myLeakyReLU()\n","        self.fc3 = myLinear(128, 10)\n","\n","  def forward(self, x):\n","        # 輸入維度從pyTorch batch計算轉換成chain rule公式形式\n","        x = torch.flatten(x, 1).T # [batch, gray, x, y] => [x*y, batch]\n","        x = self.fc1.forward(x)\n","        x = self.relu1.forward(x)\n","        x = self.dropout1.forward(x, self.train_state)\n","        x = self.fc2.forward(x)\n","        x = self.relu2.forward(x)\n","        x = self.fc3.forward(x)\n","        return x"],"metadata":{"id":"Iru4HXwfdHqi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 定義優化相關結構"],"metadata":{"id":"_CvgruWpmZ77"}},{"cell_type":"code","source":["# 定義Optimizer類\n","\n","# 抽象類\n","class Optimizer:\n","    def __init__(self, learning_rate):\n","        self.learning_rate = learning_rate\n","\n","    def step(self, layers):\n","        for layer in layers:\n","            if not hasattr(layer, 'is_WeightLayer'): # 只須更新有權重的層\n","              continue\n","            update(layer)\n","\n","    # 將更新邏輯抽象化，子類各自實現\n","    def update(layer):\n","        raise NotImplementedError(\"Subclasses should implement this!\")\n","\n","# 繼承類\n","\n","# SGD\n","class SGD(Optimizer):\n","    def __init__(self, learning_rate):\n","        super().__init__(learning_rate)\n","\n","    def update(self, layer):\n","        layer.W -= self.learning_rate * layer.dW\n","        layer.b -= self.learning_rate * layer.db\n","\n","# Momentom (從chatGPT生成的更改)\n","class Momentum(Optimizer):\n","    def __init__(self, learning_rate, momentum=0.9):\n","        super().__init__(learning_rate)\n","        self.momentum = momentum\n","        self.velocities = {}\n","\n","    def update(self, layer):\n","        if layer not in self.velocities:\n","            self.velocities[layer] = {\"W\": torch.zeros_like(layer.W, dtype=layer.W.dtype), \"b\": torch.zeros_like(layer.b, dtype=layer.b.dtype)}\n","        v = self.velocities[layer]\n","        # 更新動量\n","        v[\"W\"] = self.momentum * v[\"W\"] + self.learning_rate * layer.dW\n","        v[\"b\"] = self.momentum * v[\"b\"] + self.learning_rate * layer.db\n","        # 更新參數\n","        layer.W -= v[\"W\"]\n","        layer.b -= v[\"b\"]\n","\n","\n","# Adam (從chatGPT生成的更改)\n","class Adam(Optimizer):\n","    def __init__(self, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8):\n","        super().__init__(learning_rate)\n","        self.beta1 = beta1\n","        self.beta2 = beta2\n","        self.epsilon = epsilon\n","        self.m = {}  # 一階矩\n","        self.v = {}  # 二階矩\n","        self.t = 0  # 時間步長\n","\n","    def update(self, layer, batch_size):\n","        # 初始化一階矩和二階矩\n","        if layer not in self.m:\n","            self.m[layer] = {\"W\": torch.zeros_like(layer.W), \"b\": torch.zeros_like(layer.b)}\n","            self.v[layer] = {\"W\": torch.zeros_like(layer.W), \"b\": torch.zeros_like(layer.b)}\n","\n","        # 更新一階矩和二階矩\n","        self.m[layer][\"W\"] = self.beta1 * self.m[layer][\"W\"] + (1 - self.beta1) * layer.dW\n","        self.m[layer][\"b\"] = self.beta1 * self.m[layer][\"b\"] + (1 - self.beta1) * layer.db\n","        self.v[layer][\"W\"] = self.beta2 * self.v[layer][\"W\"] + (1 - self.beta2) * layer.dW ** 2\n","        self.v[layer][\"b\"] = self.beta2 * self.v[layer][\"b\"] + (1 - self.beta2) * layer.db ** 2\n","\n","        # 偏置修正\n","        m_hat_W = self.m[layer][\"W\"] / (1 - self.beta1 ** self.t)\n","        m_hat_b = self.m[layer][\"b\"] / (1 - self.beta1 ** self.t)\n","        v_hat_W = self.v[layer][\"W\"] / (1 - self.beta2 ** self.t)\n","        v_hat_b = self.v[layer][\"b\"] / (1 - self.beta2 ** self.t)\n","\n","        # 更新參數\n","        layer.W -= self.learning_rate * m_hat_W / (torch.sqrt(v_hat_W) + self.epsilon)\n","        layer.b -= self.learning_rate * m_hat_b / (torch.sqrt(v_hat_b) + self.epsilon)\n"],"metadata":{"id":"DL1Dy6d3OyOp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 定義損失函數\n","\n","# MSELoss\n","def mse_loss(outputs, targets):\n","    error = outputs - targets\n","    loss = torch.mean(torch.square(error))/2\n","    grad = error\n","    return loss, grad\n","\n","# CrossEntropyLoss (從chatGPT生成的更改)\n","def softmax(logits):\n","    # 防止溢出，减去最大值\n","    exp_logits = torch.exp(logits - torch.max(logits, dim=1, keepdim=True)[0]) # 取最大值并防止溢出\n","    return exp_logits / torch.sum(exp_logits, dim=1, keepdim=True)  # 使用 torch.sum 计算归一化\n","\n","def cross_entropy_loss(outputs, targets):\n","    # 计算 softmax\n","    probs = softmax(outputs.T) # 輸入維度從pyTorch batch計算轉換成chain rule公式形式\n","\n","    # 创建 one-hot 编码 (修正索引方式)\n","    num_samples = targets.shape[0]\n","    num_classes = probs.shape[1]\n","    targets_one_hot = torch.zeros(num_samples, num_classes, device=outputs.device, dtype=outputs.dtype)\n","    targets_one_hot.scatter_(1, targets.unsqueeze(1), 1)  # 使用 scatter_ 创建 one-hot 编码\n","\n","    # 计算交叉熵损失\n","    loss = -torch.sum(targets_one_hot * torch.log(probs + 1e-15)) / num_samples\n","    grad = probs - targets_one_hot\n","    return loss, grad\n"],"metadata":{"id":"Li9jYbqlOyOp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 第六版\n","    1.所有層都繼承自myModule類\n","    2.將註冊機制改為遞迴,移除isLayer,isWeightLayer\n","    3.將.to與.set_train_state方法改為遞迴\n","    4.更改dropout邏輯(判斷自己的參數)\n","    5.承1.，移除AF抽象類\n","    6.新增myModule更新參數的回傳邏輯，更改Optimizer遍歷邏輯"],"metadata":{"id":"yGmdm4yCEDdL"}},{"cell_type":"markdown","source":["### 定義基礎抽象類"],"metadata":{"id":"U9o7f5KxELMo"}},{"cell_type":"code","source":["# 定義Module抽象類\n","class myModule():\n","  def __init__(self):\n","        self.train_state = True\n","        self.sub_layers = []\n","\n","  def set_train_state(self, state):\n","        self.train_state = state\n","        for layer in self.sub_layers:\n","          layer.set_train_state(state)\n","\n","  def forward(self, x):\n","        raise NotImplementedError(\"Subclasses should implement this!\")\n","\n","  def backward(self, error):\n","        raise NotImplementedError(\"Subclasses should implement this!\")\n","\n","  def get_layers(self):\n","        return self.sub_layers\n","\n","  def to(self, device):\n","        for layer in self.sub_layers:\n","          layer.to(device)\n","\n","        return self\n","\n","  def parameters(self):\n","        params = []\n","        for layer in self.sub_layers:\n","            params.extend(layer.parameters())\n","        return params\n","\n","  def __setattr__(self, name, value):\n","        if isinstance(value, myModule):  # 有 is_layer 屬性才加入layers\n","          if value not in self.sub_layers:  # 避免重複加入\n","            self.sub_layers.append(value)\n","\n","        super().__setattr__(name, value) # 預設方法、建立任何子物件都要執行"],"metadata":{"id":"IzKXE67kEXaR","executionInfo":{"status":"ok","timestamp":1735194897359,"user_tz":-480,"elapsed":279,"user":{"displayName":"杜承祐","userId":"11358882679040380442"}}},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":["### 定義layer結構"],"metadata":{"id":"AT-63ZJeEDdM"}},{"cell_type":"code","source":["# 定義全連接層結構\n","class myLinear(myModule):\n","  def __init__(self, input_size, output_size, device='cpu',dtype=torch.float64):\n","\n","        super().__init__()\n","        # 初始化參數\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.device = device\n","        self.dtype = dtype\n","\n","        # 用于儲存梯度\n","        self.x = None\n","        self.dW = None\n","        self.db = None\n","\n","        # 初始化權重並移到設備\n","        #self.W = torch.randn(output_size, input_size, device=self.device, dtype=self.dtype)\n","        self.W = 2 * torch.rand(output_size, input_size, device=self.device, dtype=self.dtype) - 1 # -1到1之間均勻分布\n","        self.b = torch.zeros(output_size, 1, device=self.device, dtype=self.dtype)\n","        return\n","\n","  def forward(self, x):\n","        self.x = x.to(self.device, dtype=self.dtype) # 儲存本次計算結果\n","        return torch.matmul(self.W, self.x) + self.b\n","\n","  def backward(self, error):\n","        # 計算batch_size\n","        batch_size = error.size(1)\n","\n","        # 計算自己的更新梯度 (將batch計算邏輯移到此處)\n","        self.dW = torch.matmul(error, self.x.T)/batch_size\n","        self.db = torch.sum(error, dim=1, keepdim=True)/batch_size\n","\n","        # 梯度傳遞\n","        gradient_transfer = torch.matmul(self.W.T, error)\n","        return gradient_transfer\n","\n","  def to(self, device):\n","        # 将参数轉移到設備\n","        self.device = device\n","        self.W = self.W.to(device)\n","        self.b = self.b.to(device)\n","\n","  def parameters(self):\n","        return [[self.W, self.dW], [self.b, self.db]]"],"metadata":{"id":"oYCuvN0-EDdM","executionInfo":{"status":"ok","timestamp":1735194898200,"user_tz":-480,"elapsed":569,"user":{"displayName":"杜承祐","userId":"11358882679040380442"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["# 定義dropout層\n","class myDropout(myModule):\n","  def __init__(self, p=0.5, dtype=torch.float64):\n","        super().__init__()\n","\n","        self.p = p\n","        self.mask = None\n","        self.dtype = dtype\n","        return\n","\n","  def forward(self, x):\n","        if not self.train_state:\n","            return x  # 測試時直接返回輸入\n","\n","        # 隨機生成遮罩\n","        self.mask = torch.rand_like(x, dtype=self.dtype) > self.p\n","        # 遮罩應用並進行縮放\n","        return x * self.mask / (1 - self.p)\n","\n","  def backward(self, error):\n","\n","        # 只保留激活的部分\n","        return error * self.mask\n"],"metadata":{"id":"Pj6Q2pmaEDdM","executionInfo":{"status":"ok","timestamp":1735194898200,"user_tz":-480,"elapsed":2,"user":{"displayName":"杜承祐","userId":"11358882679040380442"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["# 定義activation function類\n","\n","# ReLu\n","class myReLU(myModule):\n","  def __init__(self):\n","        super().__init__()\n","        self.x = None\n","        return\n","\n","  def forward(self, x):\n","        # ReLU(x) = x>0 ? x : 0\n","        self.x = x\n","        return torch.maximum(torch.tensor(0.0, dtype=x.dtype).to(x.device), x)\n","\n","  def backward(self, error):\n","        # ReLU(x)' = x>0 ? 1:0\n","        return error * (self.x > 0).float()\n","\n","# LeakyReLU\n","class myLeakyReLU(myModule):\n","  def __init__(self, alpha=0.01):\n","        super().__init__()\n","        self.x = None\n","        self.alpha = alpha  # 控制負區域的斜率\n","        return\n","\n","  def forward(self, x):\n","        # Leaky ReLU(x) = x>0 ? x : alpha*x\n","        self.x = x\n","        return torch.where(x > 0, x, self.alpha * x)\n","\n","  def backward(self, error):\n","        # Leaky ReLU(x)' = x > 0 ? 1 : alpha\n","        return error * torch.where(self.x > 0, torch.ones_like(self.x), self.alpha * torch.ones_like(self.x))\n","\n","# Sigmoid (使用絕對數值、歸一化到[0,1]時使用)\n","class mySigmoid(myModule):\n","    def __init__(self):\n","        super().__init__()\n","        self.output = None\n","        return\n","\n","    def forward(self, x):\n","        # self.output = 1 / (1 + np.exp(-x))\n","        self.output = torch.sigmoid(x)\n","        return self.output\n","\n","    def backward(self, error):\n","        # sigmoid(x)' = sigmoid(x) * (1 - sigmoid(x))\n","        return error * self.output * (1 - self.output)\n","\n","\n","# Tanh (使用差分、標準化到[-1,1]時使用)\n","class myTanh(myModule):\n","    def __init__(self):\n","        super().__init__()\n","        self.output = None\n","        return\n","\n","    def forward(self, x):\n","        # self.output = np.tanh(x)\n","        self.output = torch.tanh(x)\n","        return self.output\n","\n","    def backward(self, error):\n","        # tanh(x)' = 1 - tanh^2(x)\n","        return error * (1 - self.output ** 2)"],"metadata":{"id":"KpLzDDK3EDdM","executionInfo":{"status":"ok","timestamp":1735194898201,"user_tz":-480,"elapsed":3,"user":{"displayName":"杜承祐","userId":"11358882679040380442"}}},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":["### 定義Net結構"],"metadata":{"id":"lKEagW2_EDdN"}},{"cell_type":"code","source":["# 定義Net架構繼承Module類\n","class myNet(myModule):\n","  def __init__(self):\n","        super().__init__()\n","        self.fc1 = myLinear(28*28, 512)\n","        self.relu1 = myLeakyReLU()\n","        self.dropout1 = myDropout(0.2)\n","        self.fc2 = myLinear(512, 128)\n","        self.relu2 = myLeakyReLU()\n","        self.fc3 = myLinear(128, 10)\n","\n","  def forward(self, x):\n","        # 輸入維度從pyTorch batch計算轉換成chain rule公式形式\n","        x = torch.flatten(x, 1).T # [batch, gray, x, y] => [x*y, batch]\n","        x = self.fc1.forward(x)\n","        x = self.relu1.forward(x)\n","        x = self.dropout1.forward(x)\n","        x = self.fc2.forward(x)\n","        x = self.relu2.forward(x)\n","        x = self.fc3.forward(x)\n","        return x\n","\n","  def backward(self, error):\n","        # 輸入維度從pyTorch batch計算轉換成chain rule公式形式\n","        error = error.T\n","        error = self.fc3.backward(error)\n","        error = self.relu2.backward(error)\n","        error = self.fc2.backward(error)\n","        error = self.relu1.backward(error)\n","        error = self.dropout1.backward(error)\n","        error = self.fc1.backward(error)\n","        return error"],"metadata":{"id":"JyWVO68iEDdN","executionInfo":{"status":"ok","timestamp":1735194898201,"user_tz":-480,"elapsed":3,"user":{"displayName":"杜承祐","userId":"11358882679040380442"}}},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":["### 定義優化相關結構"],"metadata":{"id":"srCBB4ZBEDdN"}},{"cell_type":"code","source":["# 定義Optimizer類\n","\n","# 抽象類\n","class Optimizer:\n","    def __init__(self, learning_rate):\n","        self.learning_rate = learning_rate\n","        self.t = 0\n","\n","    def step(self, parameters_and_gradients):\n","        self.t += 1\n","        #print(len(parameters_and_gradients),end=' ')\n","        for param, grad in parameters_and_gradients:\n","            #print(f\"{param.shape} {grad.shape}\")\n","            self.update(param, grad)\n","\n","    # 將更新邏輯抽象化，子類各自實現\n","    def update(self, layer):\n","        raise NotImplementedError(\"Subclasses should implement this!\")\n","\n","# 繼承類\n","\n","# SGD\n","class SGD(Optimizer):\n","    def __init__(self, learning_rate):\n","        super().__init__(learning_rate)\n","\n","    def update(self, param, grad):\n","        # 假設 param 是層的參數 (如 W 或 b)\n","        param -= self.learning_rate * grad  # 更新規則，直接減去梯度乘以學習率\n","\n","# Momentom (從chatGPT生成的更改)\n","class Momentum(Optimizer):\n","    def __init__(self, learning_rate, momentum=0.9):\n","        super().__init__(learning_rate)\n","        self.momentum = momentum\n","        self.velocities = {}\n","\n","    def update(self, param, grad):\n","        # 初始化 velocity\n","        if param not in self.velocities:\n","            self.velocities[param] = torch.zeros_like(param, dtype=grad.dtype)\n","\n","        # 更新動量\n","        v = self.velocities[param]\n","        v = self.momentum * v + self.learning_rate * grad\n","        param -= v  # 更新參數\n","\n","\n","class Adam(Optimizer):\n","    def __init__(self, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8):\n","        super().__init__(learning_rate)\n","        self.beta1 = beta1\n","        self.beta2 = beta2\n","        self.epsilon = epsilon\n","        self.m = {}  # 一階矩\n","        self.v = {}  # 二階矩\n","        self.t = 0  # 時間步長\n","\n","    def update(self, param, grad):\n","\n","        # 初始化一階矩和二階矩\n","        if param not in self.m:\n","            self.m[param] = torch.zeros_like(param)\n","            self.v[param] = torch.zeros_like(param)\n","\n","        # 更新一階矩和二階矩\n","        self.m[param] = self.beta1 * self.m[param] + (1 - self.beta1) * grad\n","        self.v[param] = self.beta2 * self.v[param] + (1 - self.beta2) * grad ** 2\n","\n","        # 偏置修正\n","        m_hat = self.m[param] / (1 - self.beta1 ** self.t)\n","        v_hat = self.v[param] / (1 - self.beta2 ** self.t)\n","\n","        # 更新參數\n","        param -= self.learning_rate * m_hat / (torch.sqrt(v_hat) + self.epsilon)\n"],"metadata":{"id":"CM9LZKrzEDdN","executionInfo":{"status":"ok","timestamp":1735194898201,"user_tz":-480,"elapsed":2,"user":{"displayName":"杜承祐","userId":"11358882679040380442"}}},"execution_count":74,"outputs":[]},{"cell_type":"code","source":["# 定義損失函數\n","\n","# MSELoss\n","def mse_loss(outputs, targets):\n","    error = outputs - targets\n","    loss = torch.mean(torch.square(error))/2\n","    grad = error\n","    return loss, grad\n","\n","# CrossEntropyLoss (從chatGPT生成的更改)\n","def softmax(logits):\n","    # 防止溢出，减去最大值\n","    exp_logits = torch.exp(logits - torch.max(logits, dim=1, keepdim=True)[0]) # 取最大值并防止溢出\n","    return exp_logits / torch.sum(exp_logits, dim=1, keepdim=True)  # 使用 torch.sum 计算归一化\n","\n","def cross_entropy_loss(outputs, targets):\n","    # 计算 softmax\n","    probs = softmax(outputs.T) # 輸入維度從pyTorch batch計算轉換成chain rule公式形式\n","\n","    # 创建 one-hot 编码 (修正索引方式)\n","    num_samples = targets.shape[0]\n","    num_classes = probs.shape[1]\n","    targets_one_hot = torch.zeros(num_samples, num_classes, device=outputs.device, dtype=outputs.dtype)\n","    targets_one_hot.scatter_(1, targets.unsqueeze(1), 1)  # 使用 scatter_ 创建 one-hot 编码\n","\n","    # 计算交叉熵损失\n","    loss = -torch.sum(targets_one_hot * torch.log(probs + 1e-15)) / num_samples\n","    grad = probs - targets_one_hot\n","    return loss, grad\n"],"metadata":{"id":"vsG27uCLEDdN","executionInfo":{"status":"ok","timestamp":1735194898201,"user_tz":-480,"elapsed":2,"user":{"displayName":"杜承祐","userId":"11358882679040380442"}}},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":["# 測試訓練(使用第六版)"],"metadata":{"id":"CJvbT__XdCJV"}},{"cell_type":"markdown","metadata":{"id":"MssNLgkOdCJW"},"source":["## 設定參數"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"9deea936-3e35-44b7-ed54-7828848d17a4","id":"_LrPxxEHdCJW","executionInfo":{"status":"ok","timestamp":1735191363235,"user_tz":-480,"elapsed":286,"user":{"displayName":"杜承祐","userId":"11358882679040380442"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cpu'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}],"source":["# 設定參數\n","PATH_DATASETS = \"\" # 預設路徑\n","BATCH_SIZE = 1024  # 批量\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\"cuda\" if torch.cuda.is_available() else \"cpu\"\n"]},{"cell_type":"markdown","metadata":{"id":"3d3qrBZgdCJW"},"source":["## 步驟1：載入 MNIST 手寫阿拉伯數字資料"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d7c7d05d-d9b0-43ad-d7dd-3ccbb126bd85","id":"Wd9DxrZtdCJW","executionInfo":{"status":"ok","timestamp":1735191370799,"user_tz":-480,"elapsed":6385,"user":{"displayName":"杜承祐","userId":"11358882679040380442"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:00<00:00, 117MB/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting MNIST/raw/train-images-idx3-ubyte.gz to MNIST/raw\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28.9k/28.9k [00:00<00:00, 35.5MB/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting MNIST/raw/train-labels-idx1-ubyte.gz to MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1.65M/1.65M [00:00<00:00, 61.3MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4.54k/4.54k [00:00<00:00, 1.81MB/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST/raw\n","\n","torch.Size([60000, 28, 28]) torch.Size([10000, 28, 28])\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import tensorflow as tf\n","mnist = tf.keras.datasets.mnist\n","\n","# 下載 MNIST 手寫阿拉伯數字 訓練資料\n","train_ds = MNIST(PATH_DATASETS, train=True, download=True,\n","                 transform=transforms.ToTensor())\n","\n","# 下載測試資料\n","test_ds = MNIST(PATH_DATASETS, train=False, download=True,\n","                 transform=transforms.ToTensor())\n","\n","# 訓練/測試資料的維度\n","print(train_ds.data.shape, test_ds.data.shape)"]},{"cell_type":"markdown","metadata":{"id":"kgmaCQh6dCJW"},"source":["## 步驟2：建立模型結構"]},{"cell_type":"code","source":["model = myNet().to(device)\n","model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"36269841-5ebc-4bb7-befa-7eef63e573ac","id":"dvgSxGnldCJW","executionInfo":{"status":"ok","timestamp":1735195698287,"user_tz":-480,"elapsed":266,"user":{"displayName":"杜承祐","userId":"11358882679040380442"}}},"execution_count":85,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<__main__.myNet at 0x77fef8bf7a30>"]},"metadata":{},"execution_count":85}]},{"cell_type":"markdown","metadata":{"id":"cW9IQY7sdCJX"},"source":["## 步驟3：結合訓練資料及模型，進行模型訓練"]},{"cell_type":"code","source":["#based on ver.6\n","\n","epochs = 20\n","lr=0.002\n","batch_size=128\n","\n","# 建立 DataLoader\n","train_loader = DataLoader(train_ds, batch_size=batch_size)\n","\n","# 設定優化器(optimizer)\n","#optimizer = SGD(lr)\n","#optimizer = Momentum(lr,momentum=0.9)\n","optimizer = Adam(lr)\n","\n","model.set_train_state(True)\n","\n","\n","# 儲存損失\n","loss_list = []\n","for epoch in range(1, epochs + 1):\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        # 確保資料是張量並遷移到 GPU\n","        data, target = data.to(device), target.to(device)\n","\n","        #optimizer.zero_grad() #no need\n","\n","        #print(data.shape)\n","        #print(data.T.shape)\n","\n","        # 前向傳播\n","        outputs = model.forward(data)\n","\n","        # 計算損失（假設均方誤差）\n","        loss, grad = cross_entropy_loss(outputs, target)\n","        #print(len(grad[0])) # == 10\n","        '''\n","        error = outputs - target\n","        loss = np.mean(np.square(error))  # 均方误差损失\n","        '''\n","\n","        # 反向傳播\n","        model.backward(grad)\n","\n","        # 更新参数\n","        optimizer.step(model.parameters())\n","\n","        # 每10个 batch 顯示一次\n","        if batch_idx % 10 == 0:\n","            loss_list.append(loss)\n","            batch = batch_idx * len(data)\n","            data_count = len(train_loader.dataset)\n","            percentage = (100. * batch_idx / len(train_loader))\n","            print(f'Epoch {epoch}: [{batch:5d} / {data_count}] ({percentage:.0f} %)' + f'  Loss: {loss:.6f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9dca5aab-7595-4f35-e3cc-bbd5f364c3cd","id":"05ur9CSGdCJX","executionInfo":{"status":"ok","timestamp":1735196200421,"user_tz":-480,"elapsed":501282,"user":{"displayName":"杜承祐","userId":"11358882679040380442"}}},"execution_count":86,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: [    0 / 60000] (0 %)  Loss: 30.143511\n","Epoch 1: [ 1280 / 60000] (2 %)  Loss: 23.672313\n","Epoch 1: [ 2560 / 60000] (4 %)  Loss: 13.594907\n","Epoch 1: [ 3840 / 60000] (6 %)  Loss: 10.011809\n","Epoch 1: [ 5120 / 60000] (9 %)  Loss: 9.119700\n","Epoch 1: [ 6400 / 60000] (11 %)  Loss: 5.488053\n","Epoch 1: [ 7680 / 60000] (13 %)  Loss: 6.303465\n","Epoch 1: [ 8960 / 60000] (15 %)  Loss: 7.422551\n","Epoch 1: [10240 / 60000] (17 %)  Loss: 6.484425\n","Epoch 1: [11520 / 60000] (19 %)  Loss: 8.371914\n","Epoch 1: [12800 / 60000] (21 %)  Loss: 7.276864\n","Epoch 1: [14080 / 60000] (23 %)  Loss: 7.073702\n","Epoch 1: [15360 / 60000] (26 %)  Loss: 3.520344\n","Epoch 1: [16640 / 60000] (28 %)  Loss: 6.349578\n","Epoch 1: [17920 / 60000] (30 %)  Loss: 5.316423\n","Epoch 1: [19200 / 60000] (32 %)  Loss: 5.718446\n","Epoch 1: [20480 / 60000] (34 %)  Loss: 4.796844\n","Epoch 1: [21760 / 60000] (36 %)  Loss: 2.730276\n","Epoch 1: [23040 / 60000] (38 %)  Loss: 5.164700\n","Epoch 1: [24320 / 60000] (41 %)  Loss: 3.737011\n","Epoch 1: [25600 / 60000] (43 %)  Loss: 4.533305\n","Epoch 1: [26880 / 60000] (45 %)  Loss: 5.415840\n","Epoch 1: [28160 / 60000] (47 %)  Loss: 5.308999\n","Epoch 1: [29440 / 60000] (49 %)  Loss: 3.841823\n","Epoch 1: [30720 / 60000] (51 %)  Loss: 5.712439\n","Epoch 1: [32000 / 60000] (53 %)  Loss: 5.946432\n","Epoch 1: [33280 / 60000] (55 %)  Loss: 6.300873\n","Epoch 1: [34560 / 60000] (58 %)  Loss: 5.349505\n","Epoch 1: [35840 / 60000] (60 %)  Loss: 4.771752\n","Epoch 1: [37120 / 60000] (62 %)  Loss: 2.709264\n","Epoch 1: [38400 / 60000] (64 %)  Loss: 2.616912\n","Epoch 1: [39680 / 60000] (66 %)  Loss: 3.560996\n","Epoch 1: [40960 / 60000] (68 %)  Loss: 3.145601\n","Epoch 1: [42240 / 60000] (70 %)  Loss: 5.295036\n","Epoch 1: [43520 / 60000] (72 %)  Loss: 1.666381\n","Epoch 1: [44800 / 60000] (75 %)  Loss: 5.070590\n","Epoch 1: [46080 / 60000] (77 %)  Loss: 4.510097\n","Epoch 1: [47360 / 60000] (79 %)  Loss: 5.813974\n","Epoch 1: [48640 / 60000] (81 %)  Loss: 2.838031\n","Epoch 1: [49920 / 60000] (83 %)  Loss: 4.303841\n","Epoch 1: [51200 / 60000] (85 %)  Loss: 3.895821\n","Epoch 1: [52480 / 60000] (87 %)  Loss: 2.140290\n","Epoch 1: [53760 / 60000] (90 %)  Loss: 3.463049\n","Epoch 1: [55040 / 60000] (92 %)  Loss: 3.242275\n","Epoch 1: [56320 / 60000] (94 %)  Loss: 2.833870\n","Epoch 1: [57600 / 60000] (96 %)  Loss: 4.791854\n","Epoch 1: [58880 / 60000] (98 %)  Loss: 1.502088\n","Epoch 2: [    0 / 60000] (0 %)  Loss: 1.210601\n","Epoch 2: [ 1280 / 60000] (2 %)  Loss: 5.111939\n","Epoch 2: [ 2560 / 60000] (4 %)  Loss: 2.615619\n","Epoch 2: [ 3840 / 60000] (6 %)  Loss: 1.687442\n","Epoch 2: [ 5120 / 60000] (9 %)  Loss: 3.680343\n","Epoch 2: [ 6400 / 60000] (11 %)  Loss: 2.643784\n","Epoch 2: [ 7680 / 60000] (13 %)  Loss: 2.378498\n","Epoch 2: [ 8960 / 60000] (15 %)  Loss: 2.182511\n","Epoch 2: [10240 / 60000] (17 %)  Loss: 3.240220\n","Epoch 2: [11520 / 60000] (19 %)  Loss: 3.490933\n","Epoch 2: [12800 / 60000] (21 %)  Loss: 3.706286\n","Epoch 2: [14080 / 60000] (23 %)  Loss: 2.398192\n","Epoch 2: [15360 / 60000] (26 %)  Loss: 1.525766\n","Epoch 2: [16640 / 60000] (28 %)  Loss: 3.716897\n","Epoch 2: [17920 / 60000] (30 %)  Loss: 2.027552\n","Epoch 2: [19200 / 60000] (32 %)  Loss: 2.382845\n","Epoch 2: [20480 / 60000] (34 %)  Loss: 1.537182\n","Epoch 2: [21760 / 60000] (36 %)  Loss: 1.452390\n","Epoch 2: [23040 / 60000] (38 %)  Loss: 2.696857\n","Epoch 2: [24320 / 60000] (41 %)  Loss: 1.929085\n","Epoch 2: [25600 / 60000] (43 %)  Loss: 1.893299\n","Epoch 2: [26880 / 60000] (45 %)  Loss: 2.208208\n","Epoch 2: [28160 / 60000] (47 %)  Loss: 1.915330\n","Epoch 2: [29440 / 60000] (49 %)  Loss: 1.405741\n","Epoch 2: [30720 / 60000] (51 %)  Loss: 3.118555\n","Epoch 2: [32000 / 60000] (53 %)  Loss: 2.627840\n","Epoch 2: [33280 / 60000] (55 %)  Loss: 2.052793\n","Epoch 2: [34560 / 60000] (58 %)  Loss: 2.507597\n","Epoch 2: [35840 / 60000] (60 %)  Loss: 2.234149\n","Epoch 2: [37120 / 60000] (62 %)  Loss: 2.512398\n","Epoch 2: [38400 / 60000] (64 %)  Loss: 1.606372\n","Epoch 2: [39680 / 60000] (66 %)  Loss: 2.709191\n","Epoch 2: [40960 / 60000] (68 %)  Loss: 1.974577\n","Epoch 2: [42240 / 60000] (70 %)  Loss: 3.516870\n","Epoch 2: [43520 / 60000] (72 %)  Loss: 1.473494\n","Epoch 2: [44800 / 60000] (75 %)  Loss: 3.785138\n","Epoch 2: [46080 / 60000] (77 %)  Loss: 1.990090\n","Epoch 2: [47360 / 60000] (79 %)  Loss: 4.149100\n","Epoch 2: [48640 / 60000] (81 %)  Loss: 2.700530\n","Epoch 2: [49920 / 60000] (83 %)  Loss: 2.397442\n","Epoch 2: [51200 / 60000] (85 %)  Loss: 2.474136\n","Epoch 2: [52480 / 60000] (87 %)  Loss: 1.134504\n","Epoch 2: [53760 / 60000] (90 %)  Loss: 2.647584\n","Epoch 2: [55040 / 60000] (92 %)  Loss: 1.022292\n","Epoch 2: [56320 / 60000] (94 %)  Loss: 1.236664\n","Epoch 2: [57600 / 60000] (96 %)  Loss: 3.999336\n","Epoch 2: [58880 / 60000] (98 %)  Loss: 0.857330\n","Epoch 3: [    0 / 60000] (0 %)  Loss: 1.633679\n","Epoch 3: [ 1280 / 60000] (2 %)  Loss: 2.759769\n","Epoch 3: [ 2560 / 60000] (4 %)  Loss: 2.307632\n","Epoch 3: [ 3840 / 60000] (6 %)  Loss: 1.686073\n","Epoch 3: [ 5120 / 60000] (9 %)  Loss: 2.664485\n","Epoch 3: [ 6400 / 60000] (11 %)  Loss: 1.599264\n","Epoch 3: [ 7680 / 60000] (13 %)  Loss: 1.807502\n","Epoch 3: [ 8960 / 60000] (15 %)  Loss: 1.241725\n","Epoch 3: [10240 / 60000] (17 %)  Loss: 1.643797\n","Epoch 3: [11520 / 60000] (19 %)  Loss: 2.666491\n","Epoch 3: [12800 / 60000] (21 %)  Loss: 3.284030\n","Epoch 3: [14080 / 60000] (23 %)  Loss: 1.856338\n","Epoch 3: [15360 / 60000] (26 %)  Loss: 1.664304\n","Epoch 3: [16640 / 60000] (28 %)  Loss: 2.045795\n","Epoch 3: [17920 / 60000] (30 %)  Loss: 0.874295\n","Epoch 3: [19200 / 60000] (32 %)  Loss: 2.167889\n","Epoch 3: [20480 / 60000] (34 %)  Loss: 1.175371\n","Epoch 3: [21760 / 60000] (36 %)  Loss: 0.711717\n","Epoch 3: [23040 / 60000] (38 %)  Loss: 1.637468\n","Epoch 3: [24320 / 60000] (41 %)  Loss: 0.877249\n","Epoch 3: [25600 / 60000] (43 %)  Loss: 1.478488\n","Epoch 3: [26880 / 60000] (45 %)  Loss: 1.056514\n","Epoch 3: [28160 / 60000] (47 %)  Loss: 1.036589\n","Epoch 3: [29440 / 60000] (49 %)  Loss: 0.938066\n","Epoch 3: [30720 / 60000] (51 %)  Loss: 2.990031\n","Epoch 3: [32000 / 60000] (53 %)  Loss: 1.885839\n","Epoch 3: [33280 / 60000] (55 %)  Loss: 1.313240\n","Epoch 3: [34560 / 60000] (58 %)  Loss: 1.357612\n","Epoch 3: [35840 / 60000] (60 %)  Loss: 2.740714\n","Epoch 3: [37120 / 60000] (62 %)  Loss: 1.881474\n","Epoch 3: [38400 / 60000] (64 %)  Loss: 1.302875\n","Epoch 3: [39680 / 60000] (66 %)  Loss: 2.042114\n","Epoch 3: [40960 / 60000] (68 %)  Loss: 1.892701\n","Epoch 3: [42240 / 60000] (70 %)  Loss: 2.433013\n","Epoch 3: [43520 / 60000] (72 %)  Loss: 1.784003\n","Epoch 3: [44800 / 60000] (75 %)  Loss: 1.932652\n","Epoch 3: [46080 / 60000] (77 %)  Loss: 2.151139\n","Epoch 3: [47360 / 60000] (79 %)  Loss: 2.439483\n","Epoch 3: [48640 / 60000] (81 %)  Loss: 1.138866\n","Epoch 3: [49920 / 60000] (83 %)  Loss: 1.188203\n","Epoch 3: [51200 / 60000] (85 %)  Loss: 1.659558\n","Epoch 3: [52480 / 60000] (87 %)  Loss: 0.658250\n","Epoch 3: [53760 / 60000] (90 %)  Loss: 1.956135\n","Epoch 3: [55040 / 60000] (92 %)  Loss: 1.133131\n","Epoch 3: [56320 / 60000] (94 %)  Loss: 1.541765\n","Epoch 3: [57600 / 60000] (96 %)  Loss: 2.302955\n","Epoch 3: [58880 / 60000] (98 %)  Loss: 0.414471\n","Epoch 4: [    0 / 60000] (0 %)  Loss: 0.872111\n","Epoch 4: [ 1280 / 60000] (2 %)  Loss: 2.163928\n","Epoch 4: [ 2560 / 60000] (4 %)  Loss: 1.647102\n","Epoch 4: [ 3840 / 60000] (6 %)  Loss: 0.512838\n","Epoch 4: [ 5120 / 60000] (9 %)  Loss: 2.851741\n","Epoch 4: [ 6400 / 60000] (11 %)  Loss: 1.682633\n","Epoch 4: [ 7680 / 60000] (13 %)  Loss: 1.505356\n","Epoch 4: [ 8960 / 60000] (15 %)  Loss: 1.641023\n","Epoch 4: [10240 / 60000] (17 %)  Loss: 2.825093\n","Epoch 4: [11520 / 60000] (19 %)  Loss: 2.112046\n","Epoch 4: [12800 / 60000] (21 %)  Loss: 1.132182\n","Epoch 4: [14080 / 60000] (23 %)  Loss: 1.680725\n","Epoch 4: [15360 / 60000] (26 %)  Loss: 1.006502\n","Epoch 4: [16640 / 60000] (28 %)  Loss: 1.216778\n","Epoch 4: [17920 / 60000] (30 %)  Loss: 1.312476\n","Epoch 4: [19200 / 60000] (32 %)  Loss: 1.098844\n","Epoch 4: [20480 / 60000] (34 %)  Loss: 1.304443\n","Epoch 4: [21760 / 60000] (36 %)  Loss: 0.248110\n","Epoch 4: [23040 / 60000] (38 %)  Loss: 0.493800\n","Epoch 4: [24320 / 60000] (41 %)  Loss: 0.217597\n","Epoch 4: [25600 / 60000] (43 %)  Loss: 1.104363\n","Epoch 4: [26880 / 60000] (45 %)  Loss: 0.536127\n","Epoch 4: [28160 / 60000] (47 %)  Loss: 1.123096\n","Epoch 4: [29440 / 60000] (49 %)  Loss: 0.673811\n","Epoch 4: [30720 / 60000] (51 %)  Loss: 1.511401\n","Epoch 4: [32000 / 60000] (53 %)  Loss: 1.773528\n","Epoch 4: [33280 / 60000] (55 %)  Loss: 1.215376\n","Epoch 4: [34560 / 60000] (58 %)  Loss: 1.256555\n","Epoch 4: [35840 / 60000] (60 %)  Loss: 1.667227\n","Epoch 4: [37120 / 60000] (62 %)  Loss: 0.930850\n","Epoch 4: [38400 / 60000] (64 %)  Loss: 1.664749\n","Epoch 4: [39680 / 60000] (66 %)  Loss: 1.370076\n","Epoch 4: [40960 / 60000] (68 %)  Loss: 1.083319\n","Epoch 4: [42240 / 60000] (70 %)  Loss: 2.127256\n","Epoch 4: [43520 / 60000] (72 %)  Loss: 1.516686\n","Epoch 4: [44800 / 60000] (75 %)  Loss: 1.745239\n","Epoch 4: [46080 / 60000] (77 %)  Loss: 0.965037\n","Epoch 4: [47360 / 60000] (79 %)  Loss: 1.588674\n","Epoch 4: [48640 / 60000] (81 %)  Loss: 1.294521\n","Epoch 4: [49920 / 60000] (83 %)  Loss: 1.693731\n","Epoch 4: [51200 / 60000] (85 %)  Loss: 1.880829\n","Epoch 4: [52480 / 60000] (87 %)  Loss: 0.656920\n","Epoch 4: [53760 / 60000] (90 %)  Loss: 1.153881\n","Epoch 4: [55040 / 60000] (92 %)  Loss: 1.606577\n","Epoch 4: [56320 / 60000] (94 %)  Loss: 0.801579\n","Epoch 4: [57600 / 60000] (96 %)  Loss: 1.264055\n","Epoch 4: [58880 / 60000] (98 %)  Loss: 0.295706\n","Epoch 5: [    0 / 60000] (0 %)  Loss: 0.690690\n","Epoch 5: [ 1280 / 60000] (2 %)  Loss: 2.171758\n","Epoch 5: [ 2560 / 60000] (4 %)  Loss: 1.006665\n","Epoch 5: [ 3840 / 60000] (6 %)  Loss: 0.160783\n","Epoch 5: [ 5120 / 60000] (9 %)  Loss: 1.550427\n","Epoch 5: [ 6400 / 60000] (11 %)  Loss: 1.243980\n","Epoch 5: [ 7680 / 60000] (13 %)  Loss: 0.901110\n","Epoch 5: [ 8960 / 60000] (15 %)  Loss: 0.491617\n","Epoch 5: [10240 / 60000] (17 %)  Loss: 2.078978\n","Epoch 5: [11520 / 60000] (19 %)  Loss: 1.381350\n","Epoch 5: [12800 / 60000] (21 %)  Loss: 1.000512\n","Epoch 5: [14080 / 60000] (23 %)  Loss: 0.583136\n","Epoch 5: [15360 / 60000] (26 %)  Loss: 0.980299\n","Epoch 5: [16640 / 60000] (28 %)  Loss: 1.575008\n","Epoch 5: [17920 / 60000] (30 %)  Loss: 1.379920\n","Epoch 5: [19200 / 60000] (32 %)  Loss: 1.264593\n","Epoch 5: [20480 / 60000] (34 %)  Loss: 0.351729\n","Epoch 5: [21760 / 60000] (36 %)  Loss: 0.589051\n","Epoch 5: [23040 / 60000] (38 %)  Loss: 0.333314\n","Epoch 5: [24320 / 60000] (41 %)  Loss: 0.771924\n","Epoch 5: [25600 / 60000] (43 %)  Loss: 0.715478\n","Epoch 5: [26880 / 60000] (45 %)  Loss: 0.993052\n","Epoch 5: [28160 / 60000] (47 %)  Loss: 0.265582\n","Epoch 5: [29440 / 60000] (49 %)  Loss: 0.531895\n","Epoch 5: [30720 / 60000] (51 %)  Loss: 0.644052\n","Epoch 5: [32000 / 60000] (53 %)  Loss: 1.205606\n","Epoch 5: [33280 / 60000] (55 %)  Loss: 1.116081\n","Epoch 5: [34560 / 60000] (58 %)  Loss: 0.868902\n","Epoch 5: [35840 / 60000] (60 %)  Loss: 1.304300\n","Epoch 5: [37120 / 60000] (62 %)  Loss: 0.830218\n","Epoch 5: [38400 / 60000] (64 %)  Loss: 0.862009\n","Epoch 5: [39680 / 60000] (66 %)  Loss: 0.485694\n","Epoch 5: [40960 / 60000] (68 %)  Loss: 1.394888\n","Epoch 5: [42240 / 60000] (70 %)  Loss: 1.052651\n","Epoch 5: [43520 / 60000] (72 %)  Loss: 0.501237\n","Epoch 5: [44800 / 60000] (75 %)  Loss: 1.617833\n","Epoch 5: [46080 / 60000] (77 %)  Loss: 1.098511\n","Epoch 5: [47360 / 60000] (79 %)  Loss: 1.971644\n","Epoch 5: [48640 / 60000] (81 %)  Loss: 0.616104\n","Epoch 5: [49920 / 60000] (83 %)  Loss: 0.567299\n","Epoch 5: [51200 / 60000] (85 %)  Loss: 1.320639\n","Epoch 5: [52480 / 60000] (87 %)  Loss: 0.275410\n","Epoch 5: [53760 / 60000] (90 %)  Loss: 1.302267\n","Epoch 5: [55040 / 60000] (92 %)  Loss: 1.288340\n","Epoch 5: [56320 / 60000] (94 %)  Loss: 0.604195\n","Epoch 5: [57600 / 60000] (96 %)  Loss: 1.303402\n","Epoch 5: [58880 / 60000] (98 %)  Loss: 0.270097\n","Epoch 6: [    0 / 60000] (0 %)  Loss: 0.826755\n","Epoch 6: [ 1280 / 60000] (2 %)  Loss: 1.195258\n","Epoch 6: [ 2560 / 60000] (4 %)  Loss: 0.680584\n","Epoch 6: [ 3840 / 60000] (6 %)  Loss: 0.200798\n","Epoch 6: [ 5120 / 60000] (9 %)  Loss: 1.419649\n","Epoch 6: [ 6400 / 60000] (11 %)  Loss: 0.722083\n","Epoch 6: [ 7680 / 60000] (13 %)  Loss: 0.833180\n","Epoch 6: [ 8960 / 60000] (15 %)  Loss: 0.757081\n","Epoch 6: [10240 / 60000] (17 %)  Loss: 1.131744\n","Epoch 6: [11520 / 60000] (19 %)  Loss: 1.134018\n","Epoch 6: [12800 / 60000] (21 %)  Loss: 0.744948\n","Epoch 6: [14080 / 60000] (23 %)  Loss: 0.985303\n","Epoch 6: [15360 / 60000] (26 %)  Loss: 0.446068\n","Epoch 6: [16640 / 60000] (28 %)  Loss: 1.569412\n","Epoch 6: [17920 / 60000] (30 %)  Loss: 1.121424\n","Epoch 6: [19200 / 60000] (32 %)  Loss: 0.729057\n","Epoch 6: [20480 / 60000] (34 %)  Loss: 0.785741\n","Epoch 6: [21760 / 60000] (36 %)  Loss: 0.268975\n","Epoch 6: [23040 / 60000] (38 %)  Loss: 0.449564\n","Epoch 6: [24320 / 60000] (41 %)  Loss: 0.179586\n","Epoch 6: [25600 / 60000] (43 %)  Loss: 1.124212\n","Epoch 6: [26880 / 60000] (45 %)  Loss: 0.555857\n","Epoch 6: [28160 / 60000] (47 %)  Loss: 0.885927\n","Epoch 6: [29440 / 60000] (49 %)  Loss: 0.268484\n","Epoch 6: [30720 / 60000] (51 %)  Loss: 0.484006\n","Epoch 6: [32000 / 60000] (53 %)  Loss: 1.016225\n","Epoch 6: [33280 / 60000] (55 %)  Loss: 1.068308\n","Epoch 6: [34560 / 60000] (58 %)  Loss: 1.574054\n","Epoch 6: [35840 / 60000] (60 %)  Loss: 0.683423\n","Epoch 6: [37120 / 60000] (62 %)  Loss: 0.485791\n","Epoch 6: [38400 / 60000] (64 %)  Loss: 0.486647\n","Epoch 6: [39680 / 60000] (66 %)  Loss: 1.120884\n","Epoch 6: [40960 / 60000] (68 %)  Loss: 0.636119\n","Epoch 6: [42240 / 60000] (70 %)  Loss: 0.983702\n","Epoch 6: [43520 / 60000] (72 %)  Loss: 0.630585\n","Epoch 6: [44800 / 60000] (75 %)  Loss: 1.200418\n","Epoch 6: [46080 / 60000] (77 %)  Loss: 0.953729\n","Epoch 6: [47360 / 60000] (79 %)  Loss: 1.112119\n","Epoch 6: [48640 / 60000] (81 %)  Loss: 0.632818\n","Epoch 6: [49920 / 60000] (83 %)  Loss: 0.312063\n","Epoch 6: [51200 / 60000] (85 %)  Loss: 1.224942\n","Epoch 6: [52480 / 60000] (87 %)  Loss: 0.109108\n","Epoch 6: [53760 / 60000] (90 %)  Loss: 1.104657\n","Epoch 6: [55040 / 60000] (92 %)  Loss: 0.451654\n","Epoch 6: [56320 / 60000] (94 %)  Loss: 0.500286\n","Epoch 6: [57600 / 60000] (96 %)  Loss: 0.279127\n","Epoch 6: [58880 / 60000] (98 %)  Loss: 0.311739\n","Epoch 7: [    0 / 60000] (0 %)  Loss: 0.563986\n","Epoch 7: [ 1280 / 60000] (2 %)  Loss: 0.964901\n","Epoch 7: [ 2560 / 60000] (4 %)  Loss: 0.703777\n","Epoch 7: [ 3840 / 60000] (6 %)  Loss: 0.099432\n","Epoch 7: [ 5120 / 60000] (9 %)  Loss: 0.614526\n","Epoch 7: [ 6400 / 60000] (11 %)  Loss: 0.446338\n","Epoch 7: [ 7680 / 60000] (13 %)  Loss: 0.373873\n","Epoch 7: [ 8960 / 60000] (15 %)  Loss: 0.433498\n","Epoch 7: [10240 / 60000] (17 %)  Loss: 1.203885\n","Epoch 7: [11520 / 60000] (19 %)  Loss: 1.158495\n","Epoch 7: [12800 / 60000] (21 %)  Loss: 0.816717\n","Epoch 7: [14080 / 60000] (23 %)  Loss: 0.317281\n","Epoch 7: [15360 / 60000] (26 %)  Loss: 0.782137\n","Epoch 7: [16640 / 60000] (28 %)  Loss: 0.995595\n","Epoch 7: [17920 / 60000] (30 %)  Loss: 1.079973\n","Epoch 7: [19200 / 60000] (32 %)  Loss: 0.864957\n","Epoch 7: [20480 / 60000] (34 %)  Loss: 0.579134\n","Epoch 7: [21760 / 60000] (36 %)  Loss: 0.061273\n","Epoch 7: [23040 / 60000] (38 %)  Loss: 0.314744\n","Epoch 7: [24320 / 60000] (41 %)  Loss: 0.096529\n","Epoch 7: [25600 / 60000] (43 %)  Loss: 0.796207\n","Epoch 7: [26880 / 60000] (45 %)  Loss: 0.075711\n","Epoch 7: [28160 / 60000] (47 %)  Loss: 0.622129\n","Epoch 7: [29440 / 60000] (49 %)  Loss: 0.152299\n","Epoch 7: [30720 / 60000] (51 %)  Loss: 0.173720\n","Epoch 7: [32000 / 60000] (53 %)  Loss: 0.765664\n","Epoch 7: [33280 / 60000] (55 %)  Loss: 0.621719\n","Epoch 7: [34560 / 60000] (58 %)  Loss: 0.633417\n","Epoch 7: [35840 / 60000] (60 %)  Loss: 0.847186\n","Epoch 7: [37120 / 60000] (62 %)  Loss: 0.419084\n","Epoch 7: [38400 / 60000] (64 %)  Loss: 0.589097\n","Epoch 7: [39680 / 60000] (66 %)  Loss: 0.716201\n","Epoch 7: [40960 / 60000] (68 %)  Loss: 0.855622\n","Epoch 7: [42240 / 60000] (70 %)  Loss: 0.668538\n","Epoch 7: [43520 / 60000] (72 %)  Loss: 1.012654\n","Epoch 7: [44800 / 60000] (75 %)  Loss: 0.729485\n","Epoch 7: [46080 / 60000] (77 %)  Loss: 0.252660\n","Epoch 7: [47360 / 60000] (79 %)  Loss: 0.501408\n","Epoch 7: [48640 / 60000] (81 %)  Loss: 0.766949\n","Epoch 7: [49920 / 60000] (83 %)  Loss: 0.345801\n","Epoch 7: [51200 / 60000] (85 %)  Loss: 0.918199\n","Epoch 7: [52480 / 60000] (87 %)  Loss: 0.076342\n","Epoch 7: [53760 / 60000] (90 %)  Loss: 0.851326\n","Epoch 7: [55040 / 60000] (92 %)  Loss: 0.226468\n","Epoch 7: [56320 / 60000] (94 %)  Loss: 0.330708\n","Epoch 7: [57600 / 60000] (96 %)  Loss: 1.118861\n","Epoch 7: [58880 / 60000] (98 %)  Loss: 0.273877\n","Epoch 8: [    0 / 60000] (0 %)  Loss: 0.620359\n","Epoch 8: [ 1280 / 60000] (2 %)  Loss: 0.909700\n","Epoch 8: [ 2560 / 60000] (4 %)  Loss: 0.225071\n","Epoch 8: [ 3840 / 60000] (6 %)  Loss: 0.157840\n","Epoch 8: [ 5120 / 60000] (9 %)  Loss: 0.396428\n","Epoch 8: [ 6400 / 60000] (11 %)  Loss: 0.712217\n","Epoch 8: [ 7680 / 60000] (13 %)  Loss: 0.353121\n","Epoch 8: [ 8960 / 60000] (15 %)  Loss: 0.359598\n","Epoch 8: [10240 / 60000] (17 %)  Loss: 0.800218\n","Epoch 8: [11520 / 60000] (19 %)  Loss: 0.735448\n","Epoch 8: [12800 / 60000] (21 %)  Loss: 0.388378\n","Epoch 8: [14080 / 60000] (23 %)  Loss: 0.933656\n","Epoch 8: [15360 / 60000] (26 %)  Loss: 0.352789\n","Epoch 8: [16640 / 60000] (28 %)  Loss: 0.531394\n","Epoch 8: [17920 / 60000] (30 %)  Loss: 0.599948\n","Epoch 8: [19200 / 60000] (32 %)  Loss: 0.436317\n","Epoch 8: [20480 / 60000] (34 %)  Loss: 0.359924\n","Epoch 8: [21760 / 60000] (36 %)  Loss: 0.134074\n","Epoch 8: [23040 / 60000] (38 %)  Loss: 0.401713\n","Epoch 8: [24320 / 60000] (41 %)  Loss: 0.120043\n","Epoch 8: [25600 / 60000] (43 %)  Loss: 0.897820\n","Epoch 8: [26880 / 60000] (45 %)  Loss: 0.434341\n","Epoch 8: [28160 / 60000] (47 %)  Loss: 0.471580\n","Epoch 8: [29440 / 60000] (49 %)  Loss: 0.342853\n","Epoch 8: [30720 / 60000] (51 %)  Loss: 0.218532\n","Epoch 8: [32000 / 60000] (53 %)  Loss: 0.535505\n","Epoch 8: [33280 / 60000] (55 %)  Loss: 0.776407\n","Epoch 8: [34560 / 60000] (58 %)  Loss: 1.172451\n","Epoch 8: [35840 / 60000] (60 %)  Loss: 0.333848\n","Epoch 8: [37120 / 60000] (62 %)  Loss: 0.394865\n","Epoch 8: [38400 / 60000] (64 %)  Loss: 0.566895\n","Epoch 8: [39680 / 60000] (66 %)  Loss: 0.535871\n","Epoch 8: [40960 / 60000] (68 %)  Loss: 0.575959\n","Epoch 8: [42240 / 60000] (70 %)  Loss: 0.546788\n","Epoch 8: [43520 / 60000] (72 %)  Loss: 0.467416\n","Epoch 8: [44800 / 60000] (75 %)  Loss: 0.408008\n","Epoch 8: [46080 / 60000] (77 %)  Loss: 0.433807\n","Epoch 8: [47360 / 60000] (79 %)  Loss: 0.921225\n","Epoch 8: [48640 / 60000] (81 %)  Loss: 0.393743\n","Epoch 8: [49920 / 60000] (83 %)  Loss: 0.313312\n","Epoch 8: [51200 / 60000] (85 %)  Loss: 0.912971\n","Epoch 8: [52480 / 60000] (87 %)  Loss: 0.067328\n","Epoch 8: [53760 / 60000] (90 %)  Loss: 0.749299\n","Epoch 8: [55040 / 60000] (92 %)  Loss: 0.576662\n","Epoch 8: [56320 / 60000] (94 %)  Loss: 0.388339\n","Epoch 8: [57600 / 60000] (96 %)  Loss: 0.349762\n","Epoch 8: [58880 / 60000] (98 %)  Loss: 0.048296\n","Epoch 9: [    0 / 60000] (0 %)  Loss: 0.324289\n","Epoch 9: [ 1280 / 60000] (2 %)  Loss: 1.193647\n","Epoch 9: [ 2560 / 60000] (4 %)  Loss: 0.465517\n","Epoch 9: [ 3840 / 60000] (6 %)  Loss: 0.105833\n","Epoch 9: [ 5120 / 60000] (9 %)  Loss: 0.454681\n","Epoch 9: [ 6400 / 60000] (11 %)  Loss: 0.456031\n","Epoch 9: [ 7680 / 60000] (13 %)  Loss: 0.311485\n","Epoch 9: [ 8960 / 60000] (15 %)  Loss: 0.350390\n","Epoch 9: [10240 / 60000] (17 %)  Loss: 1.235448\n","Epoch 9: [11520 / 60000] (19 %)  Loss: 0.631915\n","Epoch 9: [12800 / 60000] (21 %)  Loss: 0.666115\n","Epoch 9: [14080 / 60000] (23 %)  Loss: 0.518088\n","Epoch 9: [15360 / 60000] (26 %)  Loss: 0.056069\n","Epoch 9: [16640 / 60000] (28 %)  Loss: 0.448859\n","Epoch 9: [17920 / 60000] (30 %)  Loss: 0.156876\n","Epoch 9: [19200 / 60000] (32 %)  Loss: 0.657060\n","Epoch 9: [20480 / 60000] (34 %)  Loss: 0.253027\n","Epoch 9: [21760 / 60000] (36 %)  Loss: 0.054253\n","Epoch 9: [23040 / 60000] (38 %)  Loss: 0.269304\n","Epoch 9: [24320 / 60000] (41 %)  Loss: 0.109157\n","Epoch 9: [25600 / 60000] (43 %)  Loss: 0.779070\n","Epoch 9: [26880 / 60000] (45 %)  Loss: 0.417956\n","Epoch 9: [28160 / 60000] (47 %)  Loss: 0.299764\n","Epoch 9: [29440 / 60000] (49 %)  Loss: 0.312328\n","Epoch 9: [30720 / 60000] (51 %)  Loss: 0.156386\n","Epoch 9: [32000 / 60000] (53 %)  Loss: 0.189870\n","Epoch 9: [33280 / 60000] (55 %)  Loss: 0.339807\n","Epoch 9: [34560 / 60000] (58 %)  Loss: 0.557313\n","Epoch 9: [35840 / 60000] (60 %)  Loss: 0.283770\n","Epoch 9: [37120 / 60000] (62 %)  Loss: 0.287341\n","Epoch 9: [38400 / 60000] (64 %)  Loss: 0.642062\n","Epoch 9: [39680 / 60000] (66 %)  Loss: 0.464439\n","Epoch 9: [40960 / 60000] (68 %)  Loss: 0.804356\n","Epoch 9: [42240 / 60000] (70 %)  Loss: 0.428409\n","Epoch 9: [43520 / 60000] (72 %)  Loss: 0.788239\n","Epoch 9: [44800 / 60000] (75 %)  Loss: 0.484792\n","Epoch 9: [46080 / 60000] (77 %)  Loss: 0.170475\n","Epoch 9: [47360 / 60000] (79 %)  Loss: 0.586473\n","Epoch 9: [48640 / 60000] (81 %)  Loss: 0.214229\n","Epoch 9: [49920 / 60000] (83 %)  Loss: 0.595116\n","Epoch 9: [51200 / 60000] (85 %)  Loss: 0.321483\n","Epoch 9: [52480 / 60000] (87 %)  Loss: 0.266587\n","Epoch 9: [53760 / 60000] (90 %)  Loss: 0.587897\n","Epoch 9: [55040 / 60000] (92 %)  Loss: 0.285584\n","Epoch 9: [56320 / 60000] (94 %)  Loss: 0.147836\n","Epoch 9: [57600 / 60000] (96 %)  Loss: 0.600635\n","Epoch 9: [58880 / 60000] (98 %)  Loss: 0.072868\n","Epoch 10: [    0 / 60000] (0 %)  Loss: 0.223784\n","Epoch 10: [ 1280 / 60000] (2 %)  Loss: 0.960682\n","Epoch 10: [ 2560 / 60000] (4 %)  Loss: 0.391871\n","Epoch 10: [ 3840 / 60000] (6 %)  Loss: 0.209798\n","Epoch 10: [ 5120 / 60000] (9 %)  Loss: 0.204442\n","Epoch 10: [ 6400 / 60000] (11 %)  Loss: 0.270365\n","Epoch 10: [ 7680 / 60000] (13 %)  Loss: 0.206621\n","Epoch 10: [ 8960 / 60000] (15 %)  Loss: 0.180815\n","Epoch 10: [10240 / 60000] (17 %)  Loss: 0.654210\n","Epoch 10: [11520 / 60000] (19 %)  Loss: 0.447061\n","Epoch 10: [12800 / 60000] (21 %)  Loss: 0.532770\n","Epoch 10: [14080 / 60000] (23 %)  Loss: 0.234841\n","Epoch 10: [15360 / 60000] (26 %)  Loss: 0.066293\n","Epoch 10: [16640 / 60000] (28 %)  Loss: 0.198894\n","Epoch 10: [17920 / 60000] (30 %)  Loss: 0.311557\n","Epoch 10: [19200 / 60000] (32 %)  Loss: 0.318147\n","Epoch 10: [20480 / 60000] (34 %)  Loss: 0.251099\n","Epoch 10: [21760 / 60000] (36 %)  Loss: 0.035081\n","Epoch 10: [23040 / 60000] (38 %)  Loss: 0.029482\n","Epoch 10: [24320 / 60000] (41 %)  Loss: 0.157496\n","Epoch 10: [25600 / 60000] (43 %)  Loss: 0.286384\n","Epoch 10: [26880 / 60000] (45 %)  Loss: 0.414665\n","Epoch 10: [28160 / 60000] (47 %)  Loss: 0.244126\n","Epoch 10: [29440 / 60000] (49 %)  Loss: 0.138161\n","Epoch 10: [30720 / 60000] (51 %)  Loss: 0.230448\n","Epoch 10: [32000 / 60000] (53 %)  Loss: 0.371022\n","Epoch 10: [33280 / 60000] (55 %)  Loss: 0.148360\n","Epoch 10: [34560 / 60000] (58 %)  Loss: 0.275827\n","Epoch 10: [35840 / 60000] (60 %)  Loss: 0.172538\n","Epoch 10: [37120 / 60000] (62 %)  Loss: 0.128844\n","Epoch 10: [38400 / 60000] (64 %)  Loss: 0.391703\n","Epoch 10: [39680 / 60000] (66 %)  Loss: 0.251100\n","Epoch 10: [40960 / 60000] (68 %)  Loss: 0.647005\n","Epoch 10: [42240 / 60000] (70 %)  Loss: 0.259536\n","Epoch 10: [43520 / 60000] (72 %)  Loss: 0.297947\n","Epoch 10: [44800 / 60000] (75 %)  Loss: 0.977835\n","Epoch 10: [46080 / 60000] (77 %)  Loss: 0.299953\n","Epoch 10: [47360 / 60000] (79 %)  Loss: 0.527567\n","Epoch 10: [48640 / 60000] (81 %)  Loss: 0.314158\n","Epoch 10: [49920 / 60000] (83 %)  Loss: 0.123656\n","Epoch 10: [51200 / 60000] (85 %)  Loss: 0.269709\n","Epoch 10: [52480 / 60000] (87 %)  Loss: 0.026096\n","Epoch 10: [53760 / 60000] (90 %)  Loss: 0.213560\n","Epoch 10: [55040 / 60000] (92 %)  Loss: 0.427026\n","Epoch 10: [56320 / 60000] (94 %)  Loss: 0.134101\n","Epoch 10: [57600 / 60000] (96 %)  Loss: 0.409464\n","Epoch 10: [58880 / 60000] (98 %)  Loss: 0.052806\n","Epoch 11: [    0 / 60000] (0 %)  Loss: 0.309933\n","Epoch 11: [ 1280 / 60000] (2 %)  Loss: 0.612521\n","Epoch 11: [ 2560 / 60000] (4 %)  Loss: 0.467723\n","Epoch 11: [ 3840 / 60000] (6 %)  Loss: 0.113623\n","Epoch 11: [ 5120 / 60000] (9 %)  Loss: 0.536431\n","Epoch 11: [ 6400 / 60000] (11 %)  Loss: 0.398164\n","Epoch 11: [ 7680 / 60000] (13 %)  Loss: 0.367726\n","Epoch 11: [ 8960 / 60000] (15 %)  Loss: 0.129859\n","Epoch 11: [10240 / 60000] (17 %)  Loss: 0.607960\n","Epoch 11: [11520 / 60000] (19 %)  Loss: 0.324099\n","Epoch 11: [12800 / 60000] (21 %)  Loss: 0.396731\n","Epoch 11: [14080 / 60000] (23 %)  Loss: 0.272071\n","Epoch 11: [15360 / 60000] (26 %)  Loss: 0.143654\n","Epoch 11: [16640 / 60000] (28 %)  Loss: 0.190871\n","Epoch 11: [17920 / 60000] (30 %)  Loss: 0.135475\n","Epoch 11: [19200 / 60000] (32 %)  Loss: 0.326228\n","Epoch 11: [20480 / 60000] (34 %)  Loss: 0.275744\n","Epoch 11: [21760 / 60000] (36 %)  Loss: 0.005164\n","Epoch 11: [23040 / 60000] (38 %)  Loss: 0.187450\n","Epoch 11: [24320 / 60000] (41 %)  Loss: 0.132730\n","Epoch 11: [25600 / 60000] (43 %)  Loss: 0.225561\n","Epoch 11: [26880 / 60000] (45 %)  Loss: 0.025337\n","Epoch 11: [28160 / 60000] (47 %)  Loss: 0.340417\n","Epoch 11: [29440 / 60000] (49 %)  Loss: 0.115583\n","Epoch 11: [30720 / 60000] (51 %)  Loss: 0.163577\n","Epoch 11: [32000 / 60000] (53 %)  Loss: 0.473279\n","Epoch 11: [33280 / 60000] (55 %)  Loss: 0.408371\n","Epoch 11: [34560 / 60000] (58 %)  Loss: 0.278704\n","Epoch 11: [35840 / 60000] (60 %)  Loss: 0.348746\n","Epoch 11: [37120 / 60000] (62 %)  Loss: 0.063634\n","Epoch 11: [38400 / 60000] (64 %)  Loss: 0.205606\n","Epoch 11: [39680 / 60000] (66 %)  Loss: 0.336060\n","Epoch 11: [40960 / 60000] (68 %)  Loss: 0.660823\n","Epoch 11: [42240 / 60000] (70 %)  Loss: 0.110014\n","Epoch 11: [43520 / 60000] (72 %)  Loss: 0.432348\n","Epoch 11: [44800 / 60000] (75 %)  Loss: 0.224293\n","Epoch 11: [46080 / 60000] (77 %)  Loss: 0.286044\n","Epoch 11: [47360 / 60000] (79 %)  Loss: 0.506838\n","Epoch 11: [48640 / 60000] (81 %)  Loss: 0.053680\n","Epoch 11: [49920 / 60000] (83 %)  Loss: 0.210977\n","Epoch 11: [51200 / 60000] (85 %)  Loss: 0.343913\n","Epoch 11: [52480 / 60000] (87 %)  Loss: 0.004140\n","Epoch 11: [53760 / 60000] (90 %)  Loss: 0.356191\n","Epoch 11: [55040 / 60000] (92 %)  Loss: 0.205130\n","Epoch 11: [56320 / 60000] (94 %)  Loss: 0.162798\n","Epoch 11: [57600 / 60000] (96 %)  Loss: 0.358617\n","Epoch 11: [58880 / 60000] (98 %)  Loss: 0.123531\n","Epoch 12: [    0 / 60000] (0 %)  Loss: 0.354573\n","Epoch 12: [ 1280 / 60000] (2 %)  Loss: 0.543907\n","Epoch 12: [ 2560 / 60000] (4 %)  Loss: 0.281590\n","Epoch 12: [ 3840 / 60000] (6 %)  Loss: 0.115873\n","Epoch 12: [ 5120 / 60000] (9 %)  Loss: 0.341558\n","Epoch 12: [ 6400 / 60000] (11 %)  Loss: 0.399521\n","Epoch 12: [ 7680 / 60000] (13 %)  Loss: 0.149455\n","Epoch 12: [ 8960 / 60000] (15 %)  Loss: 0.075652\n","Epoch 12: [10240 / 60000] (17 %)  Loss: 0.346491\n","Epoch 12: [11520 / 60000] (19 %)  Loss: 0.184337\n","Epoch 12: [12800 / 60000] (21 %)  Loss: 0.165178\n","Epoch 12: [14080 / 60000] (23 %)  Loss: 0.291802\n","Epoch 12: [15360 / 60000] (26 %)  Loss: 0.259430\n","Epoch 12: [16640 / 60000] (28 %)  Loss: 0.233675\n","Epoch 12: [17920 / 60000] (30 %)  Loss: 0.282775\n","Epoch 12: [19200 / 60000] (32 %)  Loss: 0.177825\n","Epoch 12: [20480 / 60000] (34 %)  Loss: 0.157039\n","Epoch 12: [21760 / 60000] (36 %)  Loss: 0.087552\n","Epoch 12: [23040 / 60000] (38 %)  Loss: 0.072705\n","Epoch 12: [24320 / 60000] (41 %)  Loss: 0.304367\n","Epoch 12: [25600 / 60000] (43 %)  Loss: 0.267642\n","Epoch 12: [26880 / 60000] (45 %)  Loss: 0.266649\n","Epoch 12: [28160 / 60000] (47 %)  Loss: 0.133578\n","Epoch 12: [29440 / 60000] (49 %)  Loss: 0.164830\n","Epoch 12: [30720 / 60000] (51 %)  Loss: 0.207943\n","Epoch 12: [32000 / 60000] (53 %)  Loss: 0.481643\n","Epoch 12: [33280 / 60000] (55 %)  Loss: 0.120269\n","Epoch 12: [34560 / 60000] (58 %)  Loss: 0.141456\n","Epoch 12: [35840 / 60000] (60 %)  Loss: 0.192191\n","Epoch 12: [37120 / 60000] (62 %)  Loss: 0.269005\n","Epoch 12: [38400 / 60000] (64 %)  Loss: 0.380685\n","Epoch 12: [39680 / 60000] (66 %)  Loss: 0.266118\n","Epoch 12: [40960 / 60000] (68 %)  Loss: 0.124541\n","Epoch 12: [42240 / 60000] (70 %)  Loss: 0.118272\n","Epoch 12: [43520 / 60000] (72 %)  Loss: 0.290804\n","Epoch 12: [44800 / 60000] (75 %)  Loss: 0.130249\n","Epoch 12: [46080 / 60000] (77 %)  Loss: 0.255696\n","Epoch 12: [47360 / 60000] (79 %)  Loss: 0.182809\n","Epoch 12: [48640 / 60000] (81 %)  Loss: 0.252813\n","Epoch 12: [49920 / 60000] (83 %)  Loss: 0.203790\n","Epoch 12: [51200 / 60000] (85 %)  Loss: 0.176808\n","Epoch 12: [52480 / 60000] (87 %)  Loss: 0.061386\n","Epoch 12: [53760 / 60000] (90 %)  Loss: 0.169205\n","Epoch 12: [55040 / 60000] (92 %)  Loss: 0.354513\n","Epoch 12: [56320 / 60000] (94 %)  Loss: 0.118985\n","Epoch 12: [57600 / 60000] (96 %)  Loss: 0.211855\n","Epoch 12: [58880 / 60000] (98 %)  Loss: 0.066249\n","Epoch 13: [    0 / 60000] (0 %)  Loss: 0.284015\n","Epoch 13: [ 1280 / 60000] (2 %)  Loss: 0.549696\n","Epoch 13: [ 2560 / 60000] (4 %)  Loss: 0.146262\n","Epoch 13: [ 3840 / 60000] (6 %)  Loss: 0.002677\n","Epoch 13: [ 5120 / 60000] (9 %)  Loss: 0.181541\n","Epoch 13: [ 6400 / 60000] (11 %)  Loss: 0.305325\n","Epoch 13: [ 7680 / 60000] (13 %)  Loss: 0.199699\n","Epoch 13: [ 8960 / 60000] (15 %)  Loss: 0.031442\n","Epoch 13: [10240 / 60000] (17 %)  Loss: 0.326159\n","Epoch 13: [11520 / 60000] (19 %)  Loss: 0.314539\n","Epoch 13: [12800 / 60000] (21 %)  Loss: 0.372654\n","Epoch 13: [14080 / 60000] (23 %)  Loss: 0.260533\n","Epoch 13: [15360 / 60000] (26 %)  Loss: 0.295586\n","Epoch 13: [16640 / 60000] (28 %)  Loss: 0.123544\n","Epoch 13: [17920 / 60000] (30 %)  Loss: 0.071024\n","Epoch 13: [19200 / 60000] (32 %)  Loss: 0.341367\n","Epoch 13: [20480 / 60000] (34 %)  Loss: 0.068937\n","Epoch 13: [21760 / 60000] (36 %)  Loss: 0.004561\n","Epoch 13: [23040 / 60000] (38 %)  Loss: 0.142993\n","Epoch 13: [24320 / 60000] (41 %)  Loss: 0.034679\n","Epoch 13: [25600 / 60000] (43 %)  Loss: 0.218368\n","Epoch 13: [26880 / 60000] (45 %)  Loss: 0.074768\n","Epoch 13: [28160 / 60000] (47 %)  Loss: 0.310166\n","Epoch 13: [29440 / 60000] (49 %)  Loss: 0.017824\n","Epoch 13: [30720 / 60000] (51 %)  Loss: 0.236626\n","Epoch 13: [32000 / 60000] (53 %)  Loss: 0.086616\n","Epoch 13: [33280 / 60000] (55 %)  Loss: 0.284633\n","Epoch 13: [34560 / 60000] (58 %)  Loss: 0.254077\n","Epoch 13: [35840 / 60000] (60 %)  Loss: 0.080906\n","Epoch 13: [37120 / 60000] (62 %)  Loss: 0.218593\n","Epoch 13: [38400 / 60000] (64 %)  Loss: 0.075690\n","Epoch 13: [39680 / 60000] (66 %)  Loss: 0.148846\n","Epoch 13: [40960 / 60000] (68 %)  Loss: 0.148646\n","Epoch 13: [42240 / 60000] (70 %)  Loss: 0.319715\n","Epoch 13: [43520 / 60000] (72 %)  Loss: 0.250652\n","Epoch 13: [44800 / 60000] (75 %)  Loss: 0.176046\n","Epoch 13: [46080 / 60000] (77 %)  Loss: 0.187159\n","Epoch 13: [47360 / 60000] (79 %)  Loss: 0.306085\n","Epoch 13: [48640 / 60000] (81 %)  Loss: 0.284946\n","Epoch 13: [49920 / 60000] (83 %)  Loss: 0.133511\n","Epoch 13: [51200 / 60000] (85 %)  Loss: 0.166891\n","Epoch 13: [52480 / 60000] (87 %)  Loss: 0.065847\n","Epoch 13: [53760 / 60000] (90 %)  Loss: 0.325585\n","Epoch 13: [55040 / 60000] (92 %)  Loss: 0.188319\n","Epoch 13: [56320 / 60000] (94 %)  Loss: 0.321842\n","Epoch 13: [57600 / 60000] (96 %)  Loss: 0.167563\n","Epoch 13: [58880 / 60000] (98 %)  Loss: 0.049300\n","Epoch 14: [    0 / 60000] (0 %)  Loss: 0.193840\n","Epoch 14: [ 1280 / 60000] (2 %)  Loss: 0.393150\n","Epoch 14: [ 2560 / 60000] (4 %)  Loss: 0.203069\n","Epoch 14: [ 3840 / 60000] (6 %)  Loss: 0.007504\n","Epoch 14: [ 5120 / 60000] (9 %)  Loss: 0.128211\n","Epoch 14: [ 6400 / 60000] (11 %)  Loss: 0.075425\n","Epoch 14: [ 7680 / 60000] (13 %)  Loss: 0.138919\n","Epoch 14: [ 8960 / 60000] (15 %)  Loss: 0.021884\n","Epoch 14: [10240 / 60000] (17 %)  Loss: 0.463969\n","Epoch 14: [11520 / 60000] (19 %)  Loss: 0.123512\n","Epoch 14: [12800 / 60000] (21 %)  Loss: 0.265384\n","Epoch 14: [14080 / 60000] (23 %)  Loss: 0.156805\n","Epoch 14: [15360 / 60000] (26 %)  Loss: 0.265400\n","Epoch 14: [16640 / 60000] (28 %)  Loss: 0.139263\n","Epoch 14: [17920 / 60000] (30 %)  Loss: 0.133575\n","Epoch 14: [19200 / 60000] (32 %)  Loss: 0.216731\n","Epoch 14: [20480 / 60000] (34 %)  Loss: 0.067509\n","Epoch 14: [21760 / 60000] (36 %)  Loss: 0.047797\n","Epoch 14: [23040 / 60000] (38 %)  Loss: 0.110297\n","Epoch 14: [24320 / 60000] (41 %)  Loss: 0.111971\n","Epoch 14: [25600 / 60000] (43 %)  Loss: 0.298248\n","Epoch 14: [26880 / 60000] (45 %)  Loss: 0.249566\n","Epoch 14: [28160 / 60000] (47 %)  Loss: 0.186968\n","Epoch 14: [29440 / 60000] (49 %)  Loss: 0.043869\n","Epoch 14: [30720 / 60000] (51 %)  Loss: 0.088859\n","Epoch 14: [32000 / 60000] (53 %)  Loss: 0.116650\n","Epoch 14: [33280 / 60000] (55 %)  Loss: 0.379182\n","Epoch 14: [34560 / 60000] (58 %)  Loss: 0.243227\n","Epoch 14: [35840 / 60000] (60 %)  Loss: 0.252968\n","Epoch 14: [37120 / 60000] (62 %)  Loss: 0.114657\n","Epoch 14: [38400 / 60000] (64 %)  Loss: 0.314777\n","Epoch 14: [39680 / 60000] (66 %)  Loss: 0.232812\n","Epoch 14: [40960 / 60000] (68 %)  Loss: 0.282264\n","Epoch 14: [42240 / 60000] (70 %)  Loss: 0.123606\n","Epoch 14: [43520 / 60000] (72 %)  Loss: 0.156429\n","Epoch 14: [44800 / 60000] (75 %)  Loss: 0.261121\n","Epoch 14: [46080 / 60000] (77 %)  Loss: 0.189599\n","Epoch 14: [47360 / 60000] (79 %)  Loss: 0.257051\n","Epoch 14: [48640 / 60000] (81 %)  Loss: 0.064979\n","Epoch 14: [49920 / 60000] (83 %)  Loss: 0.118841\n","Epoch 14: [51200 / 60000] (85 %)  Loss: 0.220241\n","Epoch 14: [52480 / 60000] (87 %)  Loss: 0.170391\n","Epoch 14: [53760 / 60000] (90 %)  Loss: 0.238770\n","Epoch 14: [55040 / 60000] (92 %)  Loss: 0.245705\n","Epoch 14: [56320 / 60000] (94 %)  Loss: 0.137865\n","Epoch 14: [57600 / 60000] (96 %)  Loss: 0.159811\n","Epoch 14: [58880 / 60000] (98 %)  Loss: 0.073641\n","Epoch 15: [    0 / 60000] (0 %)  Loss: 0.093940\n","Epoch 15: [ 1280 / 60000] (2 %)  Loss: 0.404589\n","Epoch 15: [ 2560 / 60000] (4 %)  Loss: 0.224841\n","Epoch 15: [ 3840 / 60000] (6 %)  Loss: 0.020575\n","Epoch 15: [ 5120 / 60000] (9 %)  Loss: 0.117567\n","Epoch 15: [ 6400 / 60000] (11 %)  Loss: 0.118605\n","Epoch 15: [ 7680 / 60000] (13 %)  Loss: 0.033466\n","Epoch 15: [ 8960 / 60000] (15 %)  Loss: 0.332219\n","Epoch 15: [10240 / 60000] (17 %)  Loss: 0.241363\n","Epoch 15: [11520 / 60000] (19 %)  Loss: 0.141762\n","Epoch 15: [12800 / 60000] (21 %)  Loss: 0.072907\n","Epoch 15: [14080 / 60000] (23 %)  Loss: 0.081595\n","Epoch 15: [15360 / 60000] (26 %)  Loss: 0.104554\n","Epoch 15: [16640 / 60000] (28 %)  Loss: 0.104888\n","Epoch 15: [17920 / 60000] (30 %)  Loss: 0.059633\n","Epoch 15: [19200 / 60000] (32 %)  Loss: 0.032325\n","Epoch 15: [20480 / 60000] (34 %)  Loss: 0.192763\n","Epoch 15: [21760 / 60000] (36 %)  Loss: 0.007651\n","Epoch 15: [23040 / 60000] (38 %)  Loss: 0.256652\n","Epoch 15: [24320 / 60000] (41 %)  Loss: 0.129808\n","Epoch 15: [25600 / 60000] (43 %)  Loss: 0.037589\n","Epoch 15: [26880 / 60000] (45 %)  Loss: 0.066307\n","Epoch 15: [28160 / 60000] (47 %)  Loss: 0.100448\n","Epoch 15: [29440 / 60000] (49 %)  Loss: 0.076587\n","Epoch 15: [30720 / 60000] (51 %)  Loss: 0.012591\n","Epoch 15: [32000 / 60000] (53 %)  Loss: 0.178729\n","Epoch 15: [33280 / 60000] (55 %)  Loss: 0.349900\n","Epoch 15: [34560 / 60000] (58 %)  Loss: 0.045997\n","Epoch 15: [35840 / 60000] (60 %)  Loss: 0.181520\n","Epoch 15: [37120 / 60000] (62 %)  Loss: 0.046548\n","Epoch 15: [38400 / 60000] (64 %)  Loss: 0.406933\n","Epoch 15: [39680 / 60000] (66 %)  Loss: 0.151405\n","Epoch 15: [40960 / 60000] (68 %)  Loss: 0.219490\n","Epoch 15: [42240 / 60000] (70 %)  Loss: 0.199589\n","Epoch 15: [43520 / 60000] (72 %)  Loss: 0.281592\n","Epoch 15: [44800 / 60000] (75 %)  Loss: 0.187218\n","Epoch 15: [46080 / 60000] (77 %)  Loss: 0.053125\n","Epoch 15: [47360 / 60000] (79 %)  Loss: 0.183597\n","Epoch 15: [48640 / 60000] (81 %)  Loss: 0.045696\n","Epoch 15: [49920 / 60000] (83 %)  Loss: 0.193575\n","Epoch 15: [51200 / 60000] (85 %)  Loss: 0.133837\n","Epoch 15: [52480 / 60000] (87 %)  Loss: 0.092171\n","Epoch 15: [53760 / 60000] (90 %)  Loss: 0.130016\n","Epoch 15: [55040 / 60000] (92 %)  Loss: 0.036618\n","Epoch 15: [56320 / 60000] (94 %)  Loss: 0.111127\n","Epoch 15: [57600 / 60000] (96 %)  Loss: 0.191601\n","Epoch 15: [58880 / 60000] (98 %)  Loss: 0.069517\n","Epoch 16: [    0 / 60000] (0 %)  Loss: 0.148107\n","Epoch 16: [ 1280 / 60000] (2 %)  Loss: 0.356948\n","Epoch 16: [ 2560 / 60000] (4 %)  Loss: 0.246403\n","Epoch 16: [ 3840 / 60000] (6 %)  Loss: 0.020791\n","Epoch 16: [ 5120 / 60000] (9 %)  Loss: 0.135290\n","Epoch 16: [ 6400 / 60000] (11 %)  Loss: 0.076904\n","Epoch 16: [ 7680 / 60000] (13 %)  Loss: 0.075814\n","Epoch 16: [ 8960 / 60000] (15 %)  Loss: 0.063794\n","Epoch 16: [10240 / 60000] (17 %)  Loss: 0.226560\n","Epoch 16: [11520 / 60000] (19 %)  Loss: 0.069557\n","Epoch 16: [12800 / 60000] (21 %)  Loss: 0.078047\n","Epoch 16: [14080 / 60000] (23 %)  Loss: 0.052735\n","Epoch 16: [15360 / 60000] (26 %)  Loss: 0.069805\n","Epoch 16: [16640 / 60000] (28 %)  Loss: 0.156792\n","Epoch 16: [17920 / 60000] (30 %)  Loss: 0.038567\n","Epoch 16: [19200 / 60000] (32 %)  Loss: 0.110998\n","Epoch 16: [20480 / 60000] (34 %)  Loss: 0.087602\n","Epoch 16: [21760 / 60000] (36 %)  Loss: 0.046347\n","Epoch 16: [23040 / 60000] (38 %)  Loss: 0.110222\n","Epoch 16: [24320 / 60000] (41 %)  Loss: 0.089859\n","Epoch 16: [25600 / 60000] (43 %)  Loss: 0.137948\n","Epoch 16: [26880 / 60000] (45 %)  Loss: 0.054816\n","Epoch 16: [28160 / 60000] (47 %)  Loss: 0.116603\n","Epoch 16: [29440 / 60000] (49 %)  Loss: 0.003987\n","Epoch 16: [30720 / 60000] (51 %)  Loss: 0.026256\n","Epoch 16: [32000 / 60000] (53 %)  Loss: 0.289158\n","Epoch 16: [33280 / 60000] (55 %)  Loss: 0.170671\n","Epoch 16: [34560 / 60000] (58 %)  Loss: 0.071250\n","Epoch 16: [35840 / 60000] (60 %)  Loss: 0.305166\n","Epoch 16: [37120 / 60000] (62 %)  Loss: 0.039550\n","Epoch 16: [38400 / 60000] (64 %)  Loss: 0.087762\n","Epoch 16: [39680 / 60000] (66 %)  Loss: 0.220339\n","Epoch 16: [40960 / 60000] (68 %)  Loss: 0.132500\n","Epoch 16: [42240 / 60000] (70 %)  Loss: 0.064165\n","Epoch 16: [43520 / 60000] (72 %)  Loss: 0.127691\n","Epoch 16: [44800 / 60000] (75 %)  Loss: 0.190738\n","Epoch 16: [46080 / 60000] (77 %)  Loss: 0.069443\n","Epoch 16: [47360 / 60000] (79 %)  Loss: 0.453140\n","Epoch 16: [48640 / 60000] (81 %)  Loss: 0.175417\n","Epoch 16: [49920 / 60000] (83 %)  Loss: 0.023246\n","Epoch 16: [51200 / 60000] (85 %)  Loss: 0.125750\n","Epoch 16: [52480 / 60000] (87 %)  Loss: 0.026951\n","Epoch 16: [53760 / 60000] (90 %)  Loss: 0.195868\n","Epoch 16: [55040 / 60000] (92 %)  Loss: 0.047420\n","Epoch 16: [56320 / 60000] (94 %)  Loss: 0.069501\n","Epoch 16: [57600 / 60000] (96 %)  Loss: 0.178597\n","Epoch 16: [58880 / 60000] (98 %)  Loss: 0.001643\n","Epoch 17: [    0 / 60000] (0 %)  Loss: 0.131419\n","Epoch 17: [ 1280 / 60000] (2 %)  Loss: 0.273223\n","Epoch 17: [ 2560 / 60000] (4 %)  Loss: 0.077349\n","Epoch 17: [ 3840 / 60000] (6 %)  Loss: 0.043119\n","Epoch 17: [ 5120 / 60000] (9 %)  Loss: 0.083881\n","Epoch 17: [ 6400 / 60000] (11 %)  Loss: 0.042529\n","Epoch 17: [ 7680 / 60000] (13 %)  Loss: 0.121232\n","Epoch 17: [ 8960 / 60000] (15 %)  Loss: 0.100503\n","Epoch 17: [10240 / 60000] (17 %)  Loss: 0.196788\n","Epoch 17: [11520 / 60000] (19 %)  Loss: 0.208300\n","Epoch 17: [12800 / 60000] (21 %)  Loss: 0.192277\n","Epoch 17: [14080 / 60000] (23 %)  Loss: 0.095955\n","Epoch 17: [15360 / 60000] (26 %)  Loss: 0.063895\n","Epoch 17: [16640 / 60000] (28 %)  Loss: 0.083656\n","Epoch 17: [17920 / 60000] (30 %)  Loss: 0.074492\n","Epoch 17: [19200 / 60000] (32 %)  Loss: 0.094749\n","Epoch 17: [20480 / 60000] (34 %)  Loss: 0.047243\n","Epoch 17: [21760 / 60000] (36 %)  Loss: 0.010210\n","Epoch 17: [23040 / 60000] (38 %)  Loss: 0.032156\n","Epoch 17: [24320 / 60000] (41 %)  Loss: 0.122822\n","Epoch 17: [25600 / 60000] (43 %)  Loss: 0.267570\n","Epoch 17: [26880 / 60000] (45 %)  Loss: 0.091449\n","Epoch 17: [28160 / 60000] (47 %)  Loss: 0.078341\n","Epoch 17: [29440 / 60000] (49 %)  Loss: 0.032036\n","Epoch 17: [30720 / 60000] (51 %)  Loss: 0.045661\n","Epoch 17: [32000 / 60000] (53 %)  Loss: 0.072599\n","Epoch 17: [33280 / 60000] (55 %)  Loss: 0.084216\n","Epoch 17: [34560 / 60000] (58 %)  Loss: 0.125112\n","Epoch 17: [35840 / 60000] (60 %)  Loss: 0.201556\n","Epoch 17: [37120 / 60000] (62 %)  Loss: 0.036845\n","Epoch 17: [38400 / 60000] (64 %)  Loss: 0.157000\n","Epoch 17: [39680 / 60000] (66 %)  Loss: 0.071901\n","Epoch 17: [40960 / 60000] (68 %)  Loss: 0.037612\n","Epoch 17: [42240 / 60000] (70 %)  Loss: 0.083079\n","Epoch 17: [43520 / 60000] (72 %)  Loss: 0.133942\n","Epoch 17: [44800 / 60000] (75 %)  Loss: 0.316489\n","Epoch 17: [46080 / 60000] (77 %)  Loss: 0.140459\n","Epoch 17: [47360 / 60000] (79 %)  Loss: 0.069353\n","Epoch 17: [48640 / 60000] (81 %)  Loss: 0.079003\n","Epoch 17: [49920 / 60000] (83 %)  Loss: 0.071964\n","Epoch 17: [51200 / 60000] (85 %)  Loss: 0.165245\n","Epoch 17: [52480 / 60000] (87 %)  Loss: 0.010965\n","Epoch 17: [53760 / 60000] (90 %)  Loss: 0.096586\n","Epoch 17: [55040 / 60000] (92 %)  Loss: 0.041351\n","Epoch 17: [56320 / 60000] (94 %)  Loss: 0.068090\n","Epoch 17: [57600 / 60000] (96 %)  Loss: 0.231904\n","Epoch 17: [58880 / 60000] (98 %)  Loss: 0.021385\n","Epoch 18: [    0 / 60000] (0 %)  Loss: 0.135167\n","Epoch 18: [ 1280 / 60000] (2 %)  Loss: 0.370643\n","Epoch 18: [ 2560 / 60000] (4 %)  Loss: 0.083085\n","Epoch 18: [ 3840 / 60000] (6 %)  Loss: 0.022229\n","Epoch 18: [ 5120 / 60000] (9 %)  Loss: 0.249901\n","Epoch 18: [ 6400 / 60000] (11 %)  Loss: 0.116499\n","Epoch 18: [ 7680 / 60000] (13 %)  Loss: 0.064486\n","Epoch 18: [ 8960 / 60000] (15 %)  Loss: 0.045387\n","Epoch 18: [10240 / 60000] (17 %)  Loss: 0.169966\n","Epoch 18: [11520 / 60000] (19 %)  Loss: 0.086145\n","Epoch 18: [12800 / 60000] (21 %)  Loss: 0.052891\n","Epoch 18: [14080 / 60000] (23 %)  Loss: 0.049357\n","Epoch 18: [15360 / 60000] (26 %)  Loss: 0.116085\n","Epoch 18: [16640 / 60000] (28 %)  Loss: 0.131543\n","Epoch 18: [17920 / 60000] (30 %)  Loss: 0.066395\n","Epoch 18: [19200 / 60000] (32 %)  Loss: 0.047199\n","Epoch 18: [20480 / 60000] (34 %)  Loss: 0.048217\n","Epoch 18: [21760 / 60000] (36 %)  Loss: 0.012387\n","Epoch 18: [23040 / 60000] (38 %)  Loss: 0.024623\n","Epoch 18: [24320 / 60000] (41 %)  Loss: 0.010763\n","Epoch 18: [25600 / 60000] (43 %)  Loss: 0.220876\n","Epoch 18: [26880 / 60000] (45 %)  Loss: 0.030693\n","Epoch 18: [28160 / 60000] (47 %)  Loss: 0.135472\n","Epoch 18: [29440 / 60000] (49 %)  Loss: 0.113810\n","Epoch 18: [30720 / 60000] (51 %)  Loss: 0.146569\n","Epoch 18: [32000 / 60000] (53 %)  Loss: 0.068664\n","Epoch 18: [33280 / 60000] (55 %)  Loss: 0.021292\n","Epoch 18: [34560 / 60000] (58 %)  Loss: 0.058404\n","Epoch 18: [35840 / 60000] (60 %)  Loss: 0.100227\n","Epoch 18: [37120 / 60000] (62 %)  Loss: 0.053195\n","Epoch 18: [38400 / 60000] (64 %)  Loss: 0.228053\n","Epoch 18: [39680 / 60000] (66 %)  Loss: 0.074808\n","Epoch 18: [40960 / 60000] (68 %)  Loss: 0.037036\n","Epoch 18: [42240 / 60000] (70 %)  Loss: 0.131786\n","Epoch 18: [43520 / 60000] (72 %)  Loss: 0.142938\n","Epoch 18: [44800 / 60000] (75 %)  Loss: 0.081185\n","Epoch 18: [46080 / 60000] (77 %)  Loss: 0.048980\n","Epoch 18: [47360 / 60000] (79 %)  Loss: 0.159750\n","Epoch 18: [48640 / 60000] (81 %)  Loss: 0.057623\n","Epoch 18: [49920 / 60000] (83 %)  Loss: 0.041035\n","Epoch 18: [51200 / 60000] (85 %)  Loss: 0.113820\n","Epoch 18: [52480 / 60000] (87 %)  Loss: 0.030092\n","Epoch 18: [53760 / 60000] (90 %)  Loss: 0.176212\n","Epoch 18: [55040 / 60000] (92 %)  Loss: 0.042219\n","Epoch 18: [56320 / 60000] (94 %)  Loss: 0.196862\n","Epoch 18: [57600 / 60000] (96 %)  Loss: 0.100560\n","Epoch 18: [58880 / 60000] (98 %)  Loss: 0.022887\n","Epoch 19: [    0 / 60000] (0 %)  Loss: 0.047668\n","Epoch 19: [ 1280 / 60000] (2 %)  Loss: 0.244853\n","Epoch 19: [ 2560 / 60000] (4 %)  Loss: 0.190855\n","Epoch 19: [ 3840 / 60000] (6 %)  Loss: 0.014604\n","Epoch 19: [ 5120 / 60000] (9 %)  Loss: 0.030642\n","Epoch 19: [ 6400 / 60000] (11 %)  Loss: 0.045293\n","Epoch 19: [ 7680 / 60000] (13 %)  Loss: 0.134837\n","Epoch 19: [ 8960 / 60000] (15 %)  Loss: 0.048132\n","Epoch 19: [10240 / 60000] (17 %)  Loss: 0.187639\n","Epoch 19: [11520 / 60000] (19 %)  Loss: 0.121134\n","Epoch 19: [12800 / 60000] (21 %)  Loss: 0.197556\n","Epoch 19: [14080 / 60000] (23 %)  Loss: 0.188796\n","Epoch 19: [15360 / 60000] (26 %)  Loss: 0.146039\n","Epoch 19: [16640 / 60000] (28 %)  Loss: 0.194375\n","Epoch 19: [17920 / 60000] (30 %)  Loss: 0.151853\n","Epoch 19: [19200 / 60000] (32 %)  Loss: 0.138607\n","Epoch 19: [20480 / 60000] (34 %)  Loss: 0.099646\n","Epoch 19: [21760 / 60000] (36 %)  Loss: 0.004204\n","Epoch 19: [23040 / 60000] (38 %)  Loss: 0.073864\n","Epoch 19: [24320 / 60000] (41 %)  Loss: 0.058910\n","Epoch 19: [25600 / 60000] (43 %)  Loss: 0.053609\n","Epoch 19: [26880 / 60000] (45 %)  Loss: 0.015834\n","Epoch 19: [28160 / 60000] (47 %)  Loss: 0.029253\n","Epoch 19: [29440 / 60000] (49 %)  Loss: 0.033979\n","Epoch 19: [30720 / 60000] (51 %)  Loss: 0.049507\n","Epoch 19: [32000 / 60000] (53 %)  Loss: 0.168901\n","Epoch 19: [33280 / 60000] (55 %)  Loss: 0.290910\n","Epoch 19: [34560 / 60000] (58 %)  Loss: 0.136349\n","Epoch 19: [35840 / 60000] (60 %)  Loss: 0.037705\n","Epoch 19: [37120 / 60000] (62 %)  Loss: 0.076055\n","Epoch 19: [38400 / 60000] (64 %)  Loss: 0.057094\n","Epoch 19: [39680 / 60000] (66 %)  Loss: 0.127609\n","Epoch 19: [40960 / 60000] (68 %)  Loss: 0.081956\n","Epoch 19: [42240 / 60000] (70 %)  Loss: 0.059870\n","Epoch 19: [43520 / 60000] (72 %)  Loss: 0.125956\n","Epoch 19: [44800 / 60000] (75 %)  Loss: 0.127900\n","Epoch 19: [46080 / 60000] (77 %)  Loss: 0.227675\n","Epoch 19: [47360 / 60000] (79 %)  Loss: 0.061757\n","Epoch 19: [48640 / 60000] (81 %)  Loss: 0.054683\n","Epoch 19: [49920 / 60000] (83 %)  Loss: 0.134674\n","Epoch 19: [51200 / 60000] (85 %)  Loss: 0.142170\n","Epoch 19: [52480 / 60000] (87 %)  Loss: 0.030317\n","Epoch 19: [53760 / 60000] (90 %)  Loss: 0.148986\n","Epoch 19: [55040 / 60000] (92 %)  Loss: 0.078385\n","Epoch 19: [56320 / 60000] (94 %)  Loss: 0.031037\n","Epoch 19: [57600 / 60000] (96 %)  Loss: 0.179661\n","Epoch 19: [58880 / 60000] (98 %)  Loss: 0.005243\n","Epoch 20: [    0 / 60000] (0 %)  Loss: 0.076839\n","Epoch 20: [ 1280 / 60000] (2 %)  Loss: 0.102826\n","Epoch 20: [ 2560 / 60000] (4 %)  Loss: 0.070158\n","Epoch 20: [ 3840 / 60000] (6 %)  Loss: 0.041476\n","Epoch 20: [ 5120 / 60000] (9 %)  Loss: 0.039687\n","Epoch 20: [ 6400 / 60000] (11 %)  Loss: 0.150008\n","Epoch 20: [ 7680 / 60000] (13 %)  Loss: 0.043896\n","Epoch 20: [ 8960 / 60000] (15 %)  Loss: 0.010206\n","Epoch 20: [10240 / 60000] (17 %)  Loss: 0.221555\n","Epoch 20: [11520 / 60000] (19 %)  Loss: 0.269455\n","Epoch 20: [12800 / 60000] (21 %)  Loss: 0.110194\n","Epoch 20: [14080 / 60000] (23 %)  Loss: 0.018520\n","Epoch 20: [15360 / 60000] (26 %)  Loss: 0.098864\n","Epoch 20: [16640 / 60000] (28 %)  Loss: 0.082138\n","Epoch 20: [17920 / 60000] (30 %)  Loss: 0.118438\n","Epoch 20: [19200 / 60000] (32 %)  Loss: 0.019400\n","Epoch 20: [20480 / 60000] (34 %)  Loss: 0.051502\n","Epoch 20: [21760 / 60000] (36 %)  Loss: 0.006299\n","Epoch 20: [23040 / 60000] (38 %)  Loss: 0.010825\n","Epoch 20: [24320 / 60000] (41 %)  Loss: 0.014475\n","Epoch 20: [25600 / 60000] (43 %)  Loss: 0.072551\n","Epoch 20: [26880 / 60000] (45 %)  Loss: 0.058124\n","Epoch 20: [28160 / 60000] (47 %)  Loss: 0.091208\n","Epoch 20: [29440 / 60000] (49 %)  Loss: 0.108110\n","Epoch 20: [30720 / 60000] (51 %)  Loss: 0.102348\n","Epoch 20: [32000 / 60000] (53 %)  Loss: 0.047304\n","Epoch 20: [33280 / 60000] (55 %)  Loss: 0.111766\n","Epoch 20: [34560 / 60000] (58 %)  Loss: 0.143714\n","Epoch 20: [35840 / 60000] (60 %)  Loss: 0.089885\n","Epoch 20: [37120 / 60000] (62 %)  Loss: 0.035600\n","Epoch 20: [38400 / 60000] (64 %)  Loss: 0.052323\n","Epoch 20: [39680 / 60000] (66 %)  Loss: 0.164477\n","Epoch 20: [40960 / 60000] (68 %)  Loss: 0.081515\n","Epoch 20: [42240 / 60000] (70 %)  Loss: 0.052161\n","Epoch 20: [43520 / 60000] (72 %)  Loss: 0.080561\n","Epoch 20: [44800 / 60000] (75 %)  Loss: 0.056569\n","Epoch 20: [46080 / 60000] (77 %)  Loss: 0.081277\n","Epoch 20: [47360 / 60000] (79 %)  Loss: 0.088294\n","Epoch 20: [48640 / 60000] (81 %)  Loss: 0.063729\n","Epoch 20: [49920 / 60000] (83 %)  Loss: 0.075307\n","Epoch 20: [51200 / 60000] (85 %)  Loss: 0.130124\n","Epoch 20: [52480 / 60000] (87 %)  Loss: 0.030470\n","Epoch 20: [53760 / 60000] (90 %)  Loss: 0.093259\n","Epoch 20: [55040 / 60000] (92 %)  Loss: 0.024802\n","Epoch 20: [56320 / 60000] (94 %)  Loss: 0.030803\n","Epoch 20: [57600 / 60000] (96 %)  Loss: 0.043916\n","Epoch 20: [58880 / 60000] (98 %)  Loss: 0.002978\n"]}]},{"cell_type":"code","execution_count":87,"metadata":{"scrolled":false,"colab":{"base_uri":"https://localhost:8080/","height":447},"outputId":"baf174e6-4b9c-4206-c60b-0a030a0ffdd0","id":"fPmNxzAbdCJX","executionInfo":{"status":"ok","timestamp":1735196868394,"user_tz":-480,"elapsed":350,"user":{"displayName":"杜承祐","userId":"11358882679040380442"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x77fef8a54100>]"]},"metadata":{},"execution_count":87},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# 對訓練過程的損失繪圖\n","import matplotlib.pyplot as plt\n","\n","cpu_loss_list = [loss.cpu().item() for loss in loss_list]\n","plt.plot(cpu_loss_list, 'r')"]},{"cell_type":"markdown","metadata":{"id":"A0umo5TpdCJX"},"source":["##步驟4：評分(Score Model)"]},{"cell_type":"code","execution_count":88,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a3be1127-ee6b-46ae-889d-1693dd27ca29","id":"1JNXqgK6dCJX","executionInfo":{"status":"ok","timestamp":1735196871720,"user_tz":-480,"elapsed":1822,"user":{"displayName":"杜承祐","userId":"11358882679040380442"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["平均損失: 0.0003, 準確率: 9656/10000 (97%)\n","\n"]}],"source":["# 建立 DataLoader\n","test_loader = DataLoader(test_ds, batch_size=600)\n","\n","# 設置模型為評估模式\n","model.set_train_state(False)\n","\n","test_loss = 0\n","correct = 0\n","\n","# 不需要計算梯度\n","for data, target in test_loader:\n","    data, target = data.to(device), target.to(device)\n","\n","    # 前向傳播\n","    outputs = model.forward(data)\n","\n","    # 計算損失（假設均方誤差）\n","    loss, grad = cross_entropy_loss(outputs, target)\n","\n","    # 累計損失\n","    test_loss += loss\n","\n","    # 預測\n","    #pred = np.argmax(outputs, axis=0)\n","    pred = torch.argmax(outputs, dim=0)\n","    #print(pred)\n","\n","    # 計算正確筆數\n","    #correct += np.sum(pred == target.cpu().numpy())\n","    correct += torch.sum(pred == target).item()\n","\n","# 平均損失\n","test_loss /= len(test_loader.dataset)\n","\n","# 顯示測試結果\n","data_count = len(test_loader.dataset)\n","accuracy = 100. * correct / data_count\n","print(f'平均損失: {test_loss:.4f}, 準確率: {correct}/{data_count}' +\n","      f' ({accuracy:.0f}%)\\n')\n"]}]}